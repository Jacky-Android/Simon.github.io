<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="https://avatars.githubusercontent.com/u/55181594?v=4">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>训练神经网络的技巧&nbsp;|&nbsp;Simon’s Blogs</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="训练神经网络的技巧">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🤏&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="https://avatars.githubusercontent.com/u/55181594?v=4"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="road.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🤑&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>2024年深度学习学习路线</span>
        </div>
      </a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😀&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About me</span>
        </div>
      </a>
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="12.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🤏&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>训练神经网络的技巧</span>
        </div>
      </a>
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="HA.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😶&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>HAFormer复现</span>
        </div>
      </a>
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="ma.html">
        <div class="Navbar__Btn">
          
          <span>‣ </span>
        </div>
      </a>
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🤏&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">训练神经网络的技巧</h1>
    
  </header>
  <article id="https://www.notion.so/f582af0fa90340b7b8d18b2dd745ad5c" class="PageRoot"><h3 id="https://www.notion.so/2b8bd980a1444faa8b19c848d206c04d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/2b8bd980a1444faa8b19c848d206c04d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1）神经网络训练是一种有漏洞的抽象</span></span></h3><div id="https://www.notion.so/79dafa2d8cf24f668e23b42c25148f2d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">据称，开始训练神经网络非常简单。许多库和框架都以展示 30 行神奇的代码片段来解决问题而自豪，给人一种这些东西是即插即用的（错误）印象。常见的情况如下：</span></span></p></div><pre id="https://www.notion.so/a6c4e4f2106146dc9552dcee2f89924f" class="Code"><code><span class="SemanticStringArray"><span class="SemanticString"><span>&gt;&gt;&gt; your_data = # plug your awesome dataset here
&gt;&gt;&gt; model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)
# conquer world here
</span></span></span></code></pre><div id="https://www.notion.so/7c05914ed8d34b13b86e17c6ac32c8df" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这些库和示例激活了我们大脑中熟悉标准软件的部分 - 通常可以获得干净的 API 和抽象。</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://docs.python-requests.org/en/master/">请求</a></span><span class="SemanticString">库演示：</span></span></p></div><pre id="https://www.notion.so/edbc3eaf556e49afa292a6a9822893f5" class="Code"><code><span class="SemanticStringArray"><span class="SemanticString"><span>&gt;&gt;&gt; r = requests.get(&#x27;&lt;https://api.github.com/user&gt;&#x27;, auth=(&#x27;user&#x27;, &#x27;pass&#x27;))
&gt;&gt;&gt; r.status_code
200
</span></span></span></code></pre><div id="https://www.notion.so/87d97c0dab674c09848f2004883b3cb9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">太棒了！一位勇敢的开发人员已经为您承担了理解查询字符串、URL、GET/POST 请求、HTTP 连接等的负担，并将复杂性隐藏在几行代码后面。这就是我们所熟悉和期望的。不幸的是，神经网络并非如此。当您稍微偏离训练 ImageNet 分类器时，它们就不是“现成的”技术。我试图在我的帖子</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">“是的，您应该了解反向传播”</a></span><span class="SemanticString">中提出这一点，我挑选了反向传播并将其称为“泄漏的抽象”，但不幸的是，情况要糟糕得多。反向传播 + SGD 不会神奇地让您的网络工作。批量规范不会神奇地使其收敛得更快。RNN 不会神奇地让您“插入”文本。仅仅因为您可以将问题表述为 RL 并不意味着您应该这样做。如果您坚持使用该技术而不了解其工作原理，您很可能会失败。这让我想到……</span></span></p></div><h3 id="https://www.notion.so/1524ec38e46642798d7ae245e93a69b3" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/1524ec38e46642798d7ae245e93a69b3"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2）神经网络训练悄然失败</span></span></h3><div id="https://www.notion.so/05256420e3394ec6ab9632938c00f4b8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">当您破坏或错误配置代码时，您通常会遇到某种异常。您插入了一个整数，而某个函数需要字符串。该函数只需要 3 个参数。此导入失败。该键不存在。两个列表中的元素数量不相等。此外，通常可以为某个功能创建单元测试。</span></span></p></div><div id="https://www.notion.so/fe12289c07f248a1892be8ce52fc79c5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这只是训练神经网络的开始。从语法上来说，一切都可能是正确的，但整个过程并没有得到正确的安排，而且很难分辨。“可能的错误面”很大，符合逻辑（而不是语法），并且很难进行单元测试。例如，也许你在数据增强期间左右翻转图像时忘记翻转标签。你的网络仍然可以（令人震惊地）很好地工作，因为你的网络可以内部学习检测翻转的图像，然后左右翻转它的预测。或者，也许你的自回归模型由于一次错误而意外地将它试图预测的东西作为输入。或者你试图剪切梯度，但却剪切了损失，导致训练期间忽略异常值示例。或者你从预训练检查点初始化权重，但没有使用原始平均值。或者你只是搞砸了正则化强度、学习率、衰减率、模型大小等的设置。因此，只有在你幸运的情况下，配置错误的神经网络才会抛出异常；大多数时候它会进行训练，但默默地工作得更糟。</span></span></p></div><div id="https://www.notion.so/db00ab3b63884437944352db1634126d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">因此，（这一点真的很难过分强调）**“快速而激烈”的神经网络训练方法行不通，**只会导致痛苦。现在，痛苦是让神经网络良好运作的一个非常自然的部分，但可以通过彻底、防御、偏执和痴迷于可视化几乎所有可能的事情来减轻痛苦。根据我的经验，与深度学习成功最密切相关的品质是耐心和对细节的关注。</span></span></p></div><h2 id="https://www.notion.so/d01e907033224110b3bda5d97df3c54d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/d01e907033224110b3bda5d97df3c54d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">前言</span></span></h2><div id="https://www.notion.so/160d974d5be64e01a8bbd41f4090607f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">鉴于以上两个事实，我为自己开发了一个特定的流程，在将神经网络应用于新问题时我会遵循这个流程，我将尝试描述它。你会发现它非常重视上述两个原则。具体来说，它从简单到复杂，在每一步中我们都会对将会发生什么做出具体的假设，然后通过实验验证它们或进行调查，直到我们发现一些问题。我们极力避免的是一次性引入大量“未经验证”的复杂性，这必然会引入错误/错误配置，而这些错误/配置将需要很长时间才能找到（如果永远找不到的话）。如果编写神经网络代码就像训练代码一样，你会希望使用非常小的学习率，并在每次迭代后猜测然后评估完整的测试集。</span></span></p></div><h3 id="https://www.notion.so/bbde4760d3924642bb20d5307215a128" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/bbde4760d3924642bb20d5307215a128"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1\. 与数据融为一体</span></span></h3><div id="https://www.notion.so/768a61361b8349de89215c0bb0674219" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">训练神经网络的第一步是完全不接触任何神经网络代码，而是从彻底检查数据开始。这一步至关重要。我喜欢花大量时间（以小时为单位）扫描数千个示例，了解它们的分布并寻找模式。幸运的是，你的大脑在这方面非常擅长。有一次我发现数据包含重复的示例。还有一次我发现损坏的图像/标签。我寻找数据不平衡和偏差。我通常还会注意我自己对数据进行分类的过程，这暗示了我们最终将探索的架构类型。例如 - 非常局部的特征是否足够，还是我们需要全局上下文？有多少变化，它采取什么形式？哪些变化是虚假的，可以预处理掉？空间位置重要吗，还是我们想要平均池化它？细节有多重要，我们能承受多大程度的图像下采样？标签有多嘈杂？</span></span></p></div><div id="https://www.notion.so/5e1322fc832a4f9db63ce3a78fd8809e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">此外，由于神经网络实际上是数据集的压缩/编译版本，因此您将能够查看网络的（错误）预测并了解它们的来源。如果您的网络给出的预测似乎与您在数据中看到的内容不一致，则说明存在问题。</span></span></p></div><div id="https://www.notion.so/ce920f9cbde74cdaba9581934fda0c6e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">一旦你有了定性的感觉，编写一些简单的代码来搜索/过滤/排序也是个好主意，无论你想到什么（例如标签类型、注释大小、注释数量等），并可视化它们的分布和沿任何轴的异常值。尤其是异常值几乎总是能揭示数据质量或预处理中的一些错误。</span></span></p></div><h3 id="https://www.notion.so/a7419df58b8648b48770b234c836d010" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/a7419df58b8648b48770b234c836d010"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2\. 建立端到端的训练/评估框架 + 获取愚蠢的基线</span></span></h3><div id="https://www.notion.so/451df16efbcf4a8caec18154948e287a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">现在我们了解了我们的数据，我们可以使用超级花哨的多尺度 ASPP FPN ResNet 并开始训练出色的模型吗？当然不行。这是一条通往痛苦的道路。我们的下一步是建立一个完整的训练 + 评估框架，并通过一系列实验获得对其正确性的信任。在这个阶段，最好选择一些简单的模型，你不可能以某种方式搞砸它 - 例如线性分类器或非常小的 ConvNet。我们希望训练它，可视化损失、任何其他指标（例如准确性）、模型预测，并在此过程中使用明确的假设进行一系列消融实验。</span></span></p></div><div id="https://www.notion.so/7c2989ab0f6a4704a3503991cfc38fb3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">此阶段的提示和技巧：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/f19c276ad51d4e5ba6f81aa3550c0133" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">固定随机种子</strong></span><span class="SemanticString">。始终使用固定随机种子来保证当您运行代码两次时，您将获得相同的结果。这消除了变化因素并有助于让您保持理智。</span></span></li><li id="https://www.notion.so/bdc2512349dc43dcacb634e35e1c0230" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">简化</strong></span><span class="SemanticString">。确保禁用任何不必要的花哨功能。例如，在此阶段一定要关闭任何数据增强。数据增强是一种正则化策略，我们可能会在以后纳入，但现在它只是引入一些愚蠢错误的另一个机会。</span></span></li><li id="https://www.notion.so/ed1fd98029b1485ab704e9a52d734621" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">在您的 eval 中添加有效数字</strong></span><span class="SemanticString">。绘制测试损失时，对整个（大型）测试集进行评估。不要只绘制批次上的测试损失，然后依靠 Tensorboard 对其进行平滑处理。我们追求正确性，非常愿意放弃时间来保持理智。</span></span></li><li id="https://www.notion.so/b032ab464af04fc59a2690e430919669" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">验证损失 @ init</strong></span><span class="SemanticString">。验证您的损失是否从正确的损失值开始。例如，如果您正确初始化最后一层，则应</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">log(1/n_classes)</code></span><span class="SemanticString">在初始化时测量 softmax。可以为 L2 回归、Huber 损失等得出相同的默认值。</span></span></li><li id="https://www.notion.so/fb0af783ec9145a4bffbec914f127477" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">初始化好</strong></span><span class="SemanticString">。正确初始化最终层的权重。例如，如果您要回归一些平均值为 50 的值，则将最终偏差初始化为 50。如果您有一个不平衡的数据集，其正值与负值的比例为 1:10，请在您的 logits 上设置偏差，以便您的网络在初始化时预测概率为 0.1。正确设置这些将加快收敛速度并消除“曲棍球棒”损失曲线，在前几次迭代中，您的网络基本上只是在学习偏差。</span></span></li><li id="https://www.notion.so/f165f0fb122d418dbc7f03e90bc329ce" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">人类基线</strong></span><span class="SemanticString">。监控除损失之外的可人类解释和检查的指标（例如准确性）。尽可能评估您自己的（人类）准确性并与之进行比较。或者，对测试数据进行两次注释，对于每个示例，将一个注释视为预测，将第二个注释视为基本事实。</span></span></li><li id="https://www.notion.so/eea88cef565e44ec9a5f8b636a1831a5" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">独立于输入的基线</strong></span><span class="SemanticString">。训练独立于输入的基线（例如，最简单的方法是将所有输入设置为零）。这应该比您实际插入数据而不将其清零时的表现更差。是吗？即您的模型是否学会从输入中提取任何信息？</span></span></li><li id="https://www.notion.so/d50f63b38126436ab320bda56569654c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">过拟合一个批次</strong></span><span class="SemanticString">。过拟合一个只有几个示例的批次（例如只有两个）。为此，我们增加了模型的容量（例如添加层或过滤器）并验证我们是否可以达到最低的可实现损失（例如零）。我还喜欢在同一个图中可视化标签和预测，并确保一旦我们达到最小损失，它们最终会完美对齐。如果它们不对齐，则某处存在错误，我们无法继续下一阶段。</span></span></li><li id="https://www.notion.so/13d842ec0e6148dd8018b72a5aff92da" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">验证训练损失是否减少</strong></span><span class="SemanticString">。在此阶段，由于您正在使用玩具模型，因此您的数据集可能会出现欠拟合。尝试稍微增加其容量。您的训练损失是否按预期下降？</span></span></li><li id="https://www.notion.so/03ef2fd6077a48729896f7663cc39d9b" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">在网络之前进行可视化</strong></span><span class="SemanticString">。可视化数据的正确位置绝对就在网络之前</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">y_hat = model(x)</code></span><span class="SemanticString">（或</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">sess.run</code></span><span class="SemanticString">在 tf 中）。也就是说，您希望_准确地_可视化进入网络的内容，将原始数据张量和标签解码为可视化内容。这是唯一的“真相来源”。我数不清有多少次这拯救了我，并揭示了数据预处理和增强中的问题。</span></span></li><li id="https://www.notion.so/0f4595990ebe4a45978b14fd992c38ab" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">可视化预测动态</strong></span><span class="SemanticString">。我喜欢在训练过程中可视化固定测试批次上的模型预测。这些预测如何移动的“动态”将为您提供有关训练进展的非常好的直觉。很多时候，如果网络以某种方式摆动过多，就会感觉到网络“难以”适应您的数据，从而暴露出不稳定性。非常低或非常高的学习率也很容易在抖动量中被注意到。</span></span></li><li id="https://www.notion.so/26cc7b5c408b4f9e8af6994afbaae6ea" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">使用反向传播来绘制依赖关系图</strong></span><span class="SemanticString">。您的深度学习代码通常包含复杂、矢量化和广播操作。我遇到过几次的一个相对常见的错误是人们会弄错这一点（例如他们使用</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">view</code></span><span class="SemanticString">而不是</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">transpose/permute</code></span><span class="SemanticString">某处）并无意中混合了批次维度上的信息。令人沮丧的事实是，您的网络通常仍会训练正常，因为它会学会忽略来自其他示例的数据。调试此问题（和其他相关问题）的一种方法是将损失设置为一些微不足道的东西，例如示例</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">i</strong></span><span class="SemanticString">的所有输出的总和，一直向后传递到输入，并确保仅在</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">第 i 个</strong></span><span class="SemanticString">输入上获得非零梯度。相同的策略可用于例如确保您的自回归模型在时间 t 仅依赖于 1..t-1。更一般地说，梯度为您提供了有关网络中什么依赖于什么的信息，这对于调试很有用。</span></span></li><li id="https://www.notion.so/b3a16fd452344b9bbc511fbf4e4edc1d" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">概括特殊情况</strong></span><span class="SemanticString">。这更像是一个通用的编码技巧，但我经常看到人们在从头开始编写相对通用的功能时，会犯错误。我喜欢为我现在正在做的事情编写一个非常具体的函数，让它工作，然后稍后将其概括，确保得到相同的结果。这通常适用于矢量化代码，我几乎总是先写出完全循环的版本，然后一次一个循环地将其转换为矢量化代码。</span></span></li></ul><h3 id="https://www.notion.so/193e10dbccc24787900bb049307c4ce8" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/193e10dbccc24787900bb049307c4ce8"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3\. 过度拟合</span></span></h3><div id="https://www.notion.so/08b75b3adfb54123a8f6dafc5318ad06" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在这个阶段，我们应该对数据集有很好的理解，并且我们有完整的训练 + 评估流程。对于任何给定的模型，我们都可以（可重复地）计算出我们信任的指标。我们还掌握了与输入无关的基线的性能、一些愚蠢基线的性能（我们最好击败这些基线），并且我们对人类的表现有一个大致的了解（我们希望达到这一点）。现在，我们可以开始迭代一个好的模型了。</span></span></p></div><div id="https://www.notion.so/1c964eaa2b014c86b6ad17d74650e53d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我喜欢采用两个阶段来寻找好的模型：首先获得一个足够大的模型，使其能够过拟合（即专注于训练损失），然后对其进行适当的正则化（放弃一些训练损失以改善验证损失）。我喜欢这两个阶段的原因是，如果我们无法使用任何模型达到较低的错误率，这可能再次表明存在一些问题、错误或配置错误。</span></span></p></div><div id="https://www.notion.so/90ceda1a702c4d21b2e5cf03251ce92c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">此阶段的一些提示和技巧：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/e09d964e692a4f509d4b2fe37f65ae7f" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">选择模型</strong></span><span class="SemanticString">。为了达到良好的训练损失，你需要为数据选择合适的架构。在选择这个时，我的第一建议是：</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">不要当英雄</strong></span><span class="SemanticString">。我见过很多人渴望疯狂和创造性，将神经网络工具箱的乐高积木堆叠成各种对他们来说有意义的奇特架构。在项目的早期阶段要坚决抵制这种诱惑。我总是建议人们简单地找到最相关的论文并复制粘贴他们最简单的、能实现良好性能的架构。例如，如果你正在对图像进行分类，不要当英雄，只需在第一次运行时复制粘贴 ResNet-50。你以后可以做一些更自定义的事情并击败它。</span></span></li><li id="https://www.notion.so/116ec6c8720b47b5adffb4ae2c7eb514" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">adam 是安全的</strong></span><span class="SemanticString">。在设定基线的早期阶段，我喜欢使用学习率为</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://twitter.com/karpathy/status/801621764144971776?lang=en">3e-4</a></span><span class="SemanticString">的 Adam 。根据我的经验，Adam 对超参数的容忍度更高，包括糟糕的学习率。对于 ConvNets，经过良好调整的 SGD 几乎总是会略胜 Adam，但最佳学习率区域要窄得多，而且针对具体问题。（注意：如果您使用的是 RNN 和相关序列模型，则更常使用 Adam。在项目的初始阶段，再次强调，不要妄自尊大，而要遵循最相关的论文。）</span></span></li><li id="https://www.notion.so/a37b531eca554d8ca5fd29a449807a85" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">每次只复杂化一个</strong></span><span class="SemanticString">。如果您有多个信号要插入分类器，我建议您逐个插入它们，并确保每次都能获得预期的性能提升。不要一开始就将整个模型都扔到水槽里。还有其他方法可以增加复杂性 - 例如，您可以尝试先插入较小的图像，然后再将它们放大，等等。</span></span></li><li id="https://www.notion.so/aba634cf30254e0b97afd9f8bcef20db" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">不要相信学习率衰减默认值</strong></span><span class="SemanticString">。如果你正在重新利用来自其他领域的代码，请始终非常小心学习率衰减。你不仅希望针对不同的问题使用不同的衰减计划，而且 - 更糟糕的是 - 在典型的实现中，计划将基于当前的时期数，这可能会根据数据集的大小而有很大差异。例如，ImageNet 在第 30 个时期会衰减 10。如果你没有训练 ImageNet，那么你几乎肯定不想要这个。如果你不小心，你的代码可能会偷偷地将你的学习率过早地降至零，从而不允许你的模型收敛。在我自己的工作中，我总是完全禁用学习率衰减（我使用恒定的 LR）并在最后对其进行调整。</span></span></li></ul><h3 id="https://www.notion.so/54d36ddf027842db822ea258f44f85e4" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/54d36ddf027842db822ea258f44f85e4"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">4\. 正则化</span></span></h3><div id="https://www.notion.so/c35346dfcd6646dead573aed8982db70" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">理想情况下，我们现在拥有一个至少适合训练集的大型模型。现在是时候对其进行正则化并通过放弃一些训练精度来获得一些验证精度。一些提示和技巧：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/4ced2e53e00942ceb316951890277ca1" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">获取更多数据</strong></span><span class="SemanticString">。首先，在任何实际环境中，迄今为止最好的和首选的正则化模型的方法是添加更多真实训练数据。花费大量工程周期试图从小数据集中榨取价值是一个非常常见的错误，而你本可以收集更多数据。据我所知，添加更多数据几乎是无限期地单调提高配置良好的神经网络性能的唯一保证方法。另一种方法是集成（如果你能负担得起的话），但在大约 5 个模型之后就达到顶峰了。</span></span></li><li id="https://www.notion.so/3835aaba02114c8997fab4febe60a1f6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">数据增强</strong></span><span class="SemanticString">。除了真实数据之外，最好的选择就是半假数据 - 尝试更积极的数据增强。</span></span></li><li id="https://www.notion.so/0d372894250f4bf186b9e99f8ad6409b" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">创造性增强</strong></span><span class="SemanticString">。如果半假数据不起作用，假数据也可能起作用。人们正在寻找扩展数据集的创造性方法；例如，</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://openai.com/blog/learning-dexterity/">域随机化</a></span><span class="SemanticString">、使用</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://vladlen.info/publications/playing-data-ground-truth-computer-games/">模拟</a></span><span class="SemanticString">、巧妙的</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1708.01642">混合</a></span><span class="SemanticString">（例如将（潜在模拟的）数据插入场景）甚至 GAN。</span></span></li><li id="https://www.notion.so/c6dc05427bab4fe49ee9d5838cb0b75a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">预训练</strong></span><span class="SemanticString">。如果可以的话，使用预训练网络几乎不会有什么坏处，即使你有足够的数据。</span></span></li><li id="https://www.notion.so/5d32f2d0907c4f3e991c53b8742d8ffc" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">坚持监督学习</strong></span><span class="SemanticString">。不要对无监督预训练过度兴奋。与 2008 年的那篇博客文章所说的不同，据我所知，没有一个版本在现代计算机视觉中报告了强劲的结果（尽管如今 NLP 在 BERT 和朋友的帮助下似乎表现得相当不错，这很可能是由于文本的更刻意性质和更高的信噪比）。</span></span></li><li id="https://www.notion.so/f327cc0ec9334752a4ae67dd3f21521b" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">较小的输入维度</strong></span><span class="SemanticString">。删除可能包含杂散信号的特征。如果数据集较小，任何添加的杂散输入都只是过度拟合的另一个机会。同样，如果低级细节并不重要，请尝试输入较小的图像。</span></span></li><li id="https://www.notion.so/2977f989d2184624a38ed58ccca8f51c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">更小的模型尺寸</strong></span><span class="SemanticString">。在许多情况下，您可以使用网络领域的知识约束来减小其尺寸。例如，过去在 ImageNet 的主干顶部使用全连接层是一种流行做法，但这些层后来被简单的平均池化所取代，从而消除了大量的参数。</span></span></li><li id="https://www.notion.so/64801776b6c04df4ae327d5a627ad03c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">减小批量大小</strong></span><span class="SemanticString">。由于批量规范中的规范化，较小的批量大小在某种程度上对应于更强的正则化。这是因为批量经验平均值/标准差是完整平均值/标准差的更近似版本，因此比例和偏移会使您的批量“摆动”更多。</span></span></li><li id="https://www.notion.so/cc004b3e0cd046b1a10d7685a98f6797" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">drop</strong></span><span class="SemanticString">。添加 dropout。对 ConvNets 使用 dropout2d（空间 dropout）。请谨慎/小心地使用它，因为 dropout</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1801.05134">似乎</a></span><span class="SemanticString">与批量标准化不太兼容。</span></span></li><li id="https://www.notion.so/f42e0884c6df4675a62f4d33c9610dd5" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">权重衰减</strong></span><span class="SemanticString">。增加权重衰减惩罚。</span></span></li><li id="https://www.notion.so/8483bece076e461a8d88fe3ee1a70e1c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">提前停止</strong></span><span class="SemanticString">。根据测量到的验证损失停止训练，以便在模型即将过度拟合时抓住它。</span></span></li><li id="https://www.notion.so/74db0ff246044d1da3ad9f2702d65115" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">尝试更大的模型</strong></span><span class="SemanticString">。我最后提到这一点，也是在提前停止之后，但我过去曾发现过几次，较大的模型最终当然会过度拟合，但它们的“提前停止”性能通常比较小的模型好得多。</span></span></li></ul><div id="https://www.notion.so/878a69e3d17440cfbec7aab0f644a47f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">最后，为了进一步确信你的网络是一个合理的分类器，我喜欢将网络的第一层权重可视化，并确保你得到合理的边缘。如果你的第一层过滤器看起来像噪音，那么可能有些不对劲。同样，网络内的激活有时会显示奇怪的伪影并暗示存在问题。</span></span></p></div><h3 id="https://www.notion.so/519d24e87ec54456b35325aceda4f6cd" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/519d24e87ec54456b35325aceda4f6cd"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">5\. 调参</span></span></h3><div id="https://www.notion.so/b7aeac7d9c9949339d73051414ebc4a5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">现在，您应该已经“掌握”了数据集，并探索了广泛的模型空间，以找到可实现低验证损失的架构。此步骤的一些提示和技巧：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/0b87fa324cdc4f08ab6195cd82cfcb63" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">随机搜索优于网格搜索</strong></span><span class="SemanticString">。对于同时调整多个超参数，使用网格搜索来确保覆盖所有设置听起来很诱人，但请记住，最好</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf">使用随机搜索</a></span><span class="SemanticString">。直观地说，这是因为神经网络通常对某些参数比其他参数更敏感。在极限情况下，如果参数</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">a</strong></span><span class="SemanticString">很重要，但改变</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">b</strong></span><span class="SemanticString">没有效果，那么您宁愿更彻底地对</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">a</strong></span><span class="SemanticString">进行采样，而不是在几个固定点多次采样。</span></span></li><li id="https://www.notion.so/88d4f6fee8094bbfad231bfd541cefce" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">超参数优化</strong></span><span class="SemanticString">。目前有大量的贝叶斯超参数优化工具箱，我的一些朋友也报告说他们使用它们取得了成功，但我的个人经验是，探索良好而广泛的模型和超参数空间的最先进的方法是使用实习生 :)。开玩笑的。</span></span></li></ul><h3 id="https://www.notion.so/052bd6c811a54e9c8921b6764b8909e1" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/052bd6c811a54e9c8921b6764b8909e1"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">6\. 出结果</span></span></h3><div id="https://www.notion.so/f934c5607519426ba2573b6317c9219b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">一旦找到了最佳类型的架构和超参数，你仍然可以使用一些技巧来从系统中榨出最后的汁液：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/fcc8aa863481441489d8e9b3aef2eb9e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">集成</strong></span><span class="SemanticString">。模型集成几乎可以保证在任何方面获得 2% 的准确率。如果您在测试时无法承担计算量，请考虑使用</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/1503.02531">暗知识</a></span><span class="SemanticString">将您的集成提炼成网络。</span></span></li><li id="https://www.notion.so/a97fb13f46ab4ec5a54b3bcf36dba4f9" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">让它继续训练</strong></span><span class="SemanticString">。我经常看到人们在验证损失似乎趋于平稳时试图停止模型训练。根据我的经验，网络会持续训练很长时间。有一次，我在寒假期间不小心让模型继续训练，而当我在一月份回来时，它已经达到了 SOTA（“最先进的”）。</span></span></li></ul><h3 id="https://www.notion.so/94571b74cf5446b9933cbef68e12e8f1" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/94571b74cf5446b9933cbef68e12e8f1"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">结论</span></span></h3><div id="https://www.notion.so/0c0e3bec75a94abcb59ecc8ad4019a3a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">一旦你到达这里，你就拥有了成功的所有要素：你对技术、数据集和问题有着深刻的理解，你已经建立了整个训练/评估基础设施并对其准确性充满信心，并且你已经探索了越来越复杂的模型，并以你预测的方式在每一步中获得性能改进。现在你已经准备好阅读大量论文，尝试大量实验，并获得 SOTA 结果。祝你好运</span></span></p></div></article>
  <footer class="Footer">
  <div>&copy; Simon’s Blogs 2022</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>