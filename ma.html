<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="https://avatars.githubusercontent.com/u/55181594?v=4">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>Mamba博客&nbsp;|&nbsp;Simon’s Blogs</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="Mamba博客">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🐍&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="https://avatars.githubusercontent.com/u/55181594?v=4"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="road.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🤑&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>2024年深度学习学习路线</span>
        </div>
      </a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😀&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About me</span>
        </div>
      </a>
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="12.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🤏&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>训练神经网络的技巧</span>
        </div>
      </a>
    
  
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🐍&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">Mamba博客</h1>
    
  </header>
  <article id="https://www.notion.so/10e76526d1cc80b28ba5ffee53e665b5" class="PageRoot"><ul id="https://www.notion.so/10e76526d1cc80f09ecdee20c6de997a" class="ColorfulBlock ColorfulBlock--ColorGray TableOfContents"><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc80db933ffcafe04576e9"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">什么是“结构化状态空间模型”（SSM）</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc80a2b2b0c203951807c2"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString">Mamba</span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc8053acdafbfefe0ff245"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">该论文对SSM做出的贡献如下：</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc80fb9fb5cc4f8d7fe8c2"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">这两种技术的结合产生了以下特性：</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc80e58fecf00cc7f4a7fd"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">简化的SSM架构</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc805da53ce94c02922cf7"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">mamba 复现 参数解读</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc80bd9d2eea86b1c6c679"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">状态空间模型(SSM)</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc80a9b60ac5404c994c8c"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">选择性扫描算法（selective_scan）</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc804f917ac6f4b7f28566"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">视觉mamba</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc80309d5bc9936fd41acc"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Vision Mamba vs Transformers</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc803baaa2d64e8b31f997"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString">Vision Mamba编码器</span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc805b9fbfe289b58d7364"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">高分辨率图像处理</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc80da8a1ae43f53db861d"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString">Vision mamba简单复现</span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/10e76526d1cc80d2a592d36f6a1a2795"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">结论</strong></span></span></div></a></li></ul><h1 id="https://www.notion.so/10e76526d1cc80db933ffcafe04576e9" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc80db933ffcafe04576e9"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">什么是“结构化状态空间模型”（SSM）</strong></span></span></h1><div id="https://www.notion.so/10e76526d1cc80b397a7d55ce48e216a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">“结构化状态空间模型”（SSM）是Mamba的核心，因此重要的是要注意它们的工作原理。我们可以将它们视为变压器中自注意机制的替代品。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80ca921bec5fb962a581" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">状态空间模型（SSM）提供了一种结构化的方式来高效地表示和分析序列。在神经网络的上下文中，SSM可以被用作处理序列的层，其核心概念是将输入信号映射到潜在状态，然后映射到输出信号。SSM的更新和输出方程如下：</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80308accc5fbaa96cf0f" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F774a8b1f-80da-40a3-b495-2b797d3d748e%2FUntitled.png?width=342&amp;table=block&amp;id=10e76526-d1cc-8030-8acc-c5fbaa96cf0f"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F774a8b1f-80da-40a3-b495-2b797d3d748e%2FUntitled.png?width=342&amp;table=block&amp;id=10e76526-d1cc-8030-8acc-c5fbaa96cf0f" style="width:342px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc80a8b639ed8f86c67517" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这里，A、B、C和D是定义系统动态的矩阵，其</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">中A代表状态的演变方式，B代表输入对状态的影响，C代表状态如何转换为输出，而D则是直接从输入到输出的馈送。</strong></span></span></p></div><div id="https://www.notion.so/10e76526d1cc80afb382d77117e9f1e5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">需要注意的是，与标准的递归网络不同，这里的SSM是完全线性的，没有LSTM或GRU内部的非线性转换。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80bf8cdbd3143c7573c6" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F70613d02-6e3d-492e-9a45-7fe27a14b88d%2FUntitled.png?width=566.4000244140625&amp;table=block&amp;id=10e76526-d1cc-80bf-8cdb-d3143c7573c6"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F70613d02-6e3d-492e-9a45-7fe27a14b88d%2FUntitled.png?width=566.4000244140625&amp;table=block&amp;id=10e76526-d1cc-80bf-8cdb-d3143c7573c6" style="width:100%"/></a><figcaption><span class="SemanticStringArray"><span class="SemanticString">时间步 t 的隐藏状态</span></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc800e8022dc0f84a924d5" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F2f71be22-d4a2-46c1-8cfe-c0bb4c9cac0e%2FUntitled.png?width=566.4000244140625&amp;table=block&amp;id=10e76526-d1cc-800e-8022-dc0f84a924d5"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F2f71be22-d4a2-46c1-8cfe-c0bb4c9cac0e%2FUntitled.png?width=566.4000244140625&amp;table=block&amp;id=10e76526-d1cc-800e-8022-dc0f84a924d5" style="width:100%"/></a><figcaption><span class="SemanticStringArray"><span class="SemanticString">时间步 t 的输出</span></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc809abc2dffd1378028b7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在神经网络中使用SSM的直觉是将输入序列转换为一个更高维度的空间（潜在状态），在这个空间中可以更有效地捕获其动态，然后将其投影到所需的输出。矩阵 A、B 和 C 将输入数据转换为一个随时间演变的潜在空间，使模型能够捕获时间依赖关系。SSM的离散化版本使得将这种连续时间概念应用于离散时间数据（如机器学习任务中的序列）成为可能。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80b2a457de6f909cf025" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">要在离散设置中（例如神经网络训练）使用SSM，通常需要对模型进行离散化，使用类似双线性变换的方法，得到离散更新方程：</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8066bcbadb8d53687a1d" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F54e75090-abdd-489f-a925-eac25c715b6c%2FUntitled.png?width=286&amp;table=block&amp;id=10e76526-d1cc-8066-bcba-db8d53687a1d"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F54e75090-abdd-489f-a925-eac25c715b6c%2FUntitled.png?width=286&amp;table=block&amp;id=10e76526-d1cc-8066-bcba-db8d53687a1d" style="width:286px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc80c7bd2ce92ba7d0520a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这些离散方程使得SSM可以以类似于循环神经网络（RNNs）的方式应用于输入序列，但在展开时可以像卷积神经网络（CNNs）一样进行训练。这种方法可以显著提高对长序列建模的效率。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8016a202fc330f3c0de7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">SSM离散化中使用的双线性变换方法类似于决定何时拍摄这些快照，并确保连续的“电影”（如场景的运动或变化）的重要特征在这些离散的“帧”中被准确捕获。矩阵 ˉAˉ 和 ˉBˉ 是帮助我们将连续流转换为一系列步骤的工具，而不丢失我们建模过程的本质。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80e4879ce80d31952acf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">序列建模是将上下文压缩成一个较小的状态，然后使用它来预测输出序列。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80e3b2d3fc7fd48bfdf7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">注意力机制并不压缩上下文，它让模型完全访问历史记录。注意力机制可以与RNN一起使用，在过去已经这样做过，只是计算成本相当昂贵。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80558168d9145533f67f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在模型压缩状态的效率与有效性之间存在一个权衡。如果你的状态较小且上下文较少，那么模型会更有效率。如果你的状态较大且上下文较多，模型会更慢但更准确。</span></span></p></div><h1 id="https://www.notion.so/10e76526d1cc80a2b2b0c203951807c2" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc80a2b2b0c203951807c2"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Mamba</span></span></h1><h2 id="https://www.notion.so/10e76526d1cc8053acdafbfefe0ff245" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc8053acdafbfefe0ff245"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">该论文对SSM做出的贡献如下：</strong></span></span></h2><div id="https://www.notion.so/10e76526d1cc804592c9fa5f6d2cb5fd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">1. 选择机制，允许模型过滤掉不相关的信息，并无限期地记住相关信息。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8097aa9ece4364545a1e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">2. 一种硬件感知算法，用于递归计算模型，但不会在扩展状态中实现，优化GPU内存布局。</span></span></p></div><h2 id="https://www.notion.so/10e76526d1cc80fb9fb5cc4f8d7fe8c2" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc80fb9fb5cc4f8d7fe8c2"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">这两种技术的结合产生了以下特性：</strong></span></span></h2><div id="https://www.notion.so/10e76526d1cc80d8afd3ffd49c6885d6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">1.  在语言和其他具有长序列的数据上获得高质量的结果。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80a99757d39bec3945dd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">2.  快速的训练和推理。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8025ba27d0920f282abb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">3.  训练过程中内存和计算与序列长度呈线性比例。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80fea06fdd18943c7b62" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">4.  推理涉及将模型一次一个元素地展开，每个步骤的时间恒定，没有先前元素的缓存。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc805ebf2bfc7123eae3b1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">5.  长上下文——在真实数据上，性能在序列长度达到100万时有所提升。简而言之，Mamba是S4的先进版本。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80769156f4fcaaa08a43" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F5038f078-dc6c-4144-8860-29cc463e1f18%2FUntitled.png?width=566.4000244140625&amp;table=block&amp;id=10e76526-d1cc-8076-9156-f4fcaaa08a43"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F5038f078-dc6c-4144-8860-29cc463e1f18%2FUntitled.png?width=566.4000244140625&amp;table=block&amp;id=10e76526-d1cc-8076-9156-f4fcaaa08a43" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h1 id="https://www.notion.so/10e76526d1cc80e58fecf00cc7f4a7fd" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc80e58fecf00cc7f4a7fd"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">简化的SSM架构</strong></span></span></h1><div id="https://www.notion.so/10e76526d1cc80bea0a8e784dedbaee0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">可选择的SSM块可以作为独立的转换单元集成到神经网络中，就像您会将LSTM或GRU之类的RNN单元集成一样。下面是Mamba块的完整架构，它不仅仅是我们上面介绍的SSM模块。在较大的Mamba块中，SSM块周围还有线性投影、卷积和非线性变换。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80ce932bda1b76f7c81c" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fb036dd5c-5919-4acb-82f5-1255f9c85071%2FUntitled.png?width=566.4000244140625&amp;table=block&amp;id=10e76526-d1cc-80ce-932b-da1b76f7c81c"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fb036dd5c-5919-4acb-82f5-1255f9c85071%2FUntitled.png?width=566.4000244140625&amp;table=block&amp;id=10e76526-d1cc-80ce-932b-da1b76f7c81c" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc80b499cce018494b4617" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">首先，他们通过一个线性层将输入投影到高维空间，同时在右侧添加了类似于Transformer的残差连接。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8000aad0ef50d72a5aa3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">然后，他们在线性层上运行一个一维卷积，在经过SiLU/Swish激活函数之后，将其传递给我们上面讨论过的SSM块。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8095a938d669481cd652" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">残差路径然后连接回SSM的输出，并通过最终的线性层将维度缩减回与输入相同的维度。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8077b30bc06b1fc3f5d1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">一个重要的联系：RNN的经典门控机制是SSM的选择机制的一个实例。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80dca746d23d188bb902" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/10e76526d1cc805da53ce94c02922cf7" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc805da53ce94c02922cf7"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">mamba 复现 参数解读</strong></span></span></h1><ul class="BulletedListWrapper"><li id="https://www.notion.so/10e76526d1cc80c5b4faf8ce4d4d8170" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">b: 批量大小(batch size), 对应Mamba论文中algorithm 2中的B</span></span></li><li id="https://www.notion.so/10e76526d1cc80c29fb7d3dc51601941" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">l : 序列长度，对应Mamba论文中algorithm 2中的L</span></span></li><li id="https://www.notion.so/10e76526d1cc80d488fee5caa1985871" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">d或者d_model: 隐藏层的维度大小</span></span></li><li id="https://www.notion.so/10e76526d1cc80609165ca1e1b1eb9e7" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">n或者d_state: 状态维度，对应Mamba论文中algorithm 2中的N</span></span></li><li id="https://www.notion.so/10e76526d1cc801081d6c6ac546b83a3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">expand : 扩张系数，Mamba论文3.4节的E</span></span></li><li id="https://www.notion.so/10e76526d1cc8088aec2dbca3d40736d" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">d_in或者d_inner : d*expand, 对应Mamba论文中algorithm 2中的D</span></span></li><li id="https://www.notion.so/10e76526d1cc808d9798e542940ead16" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">A,B,C,D对应的是状态空间模型的参数。其中B,C是依赖于输入的，A,D并不是。</span></span></li><li id="https://www.notion.so/10e76526d1cc80caacc8fcd5c22dfe2a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Δ 或者 delta : 依赖于输入的时间步长。</span></span></li><li id="https://www.notion.so/10e76526d1cc801f8424e0f5bcab00a1" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">dt_rank: Δ的秩，对应Mamba论文中3.6节的“parameterization of Δ”
</span></span></li></ul><div id="https://www.notion.so/10e76526d1cc807a9135c365d0f06035" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F45161fdc-2cf4-459e-8ea2-98c6458e0d08%2FUntitled.png?width=707.9750366210938&amp;table=block&amp;id=10e76526-d1cc-807a-9135-c365d0f06035"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F45161fdc-2cf4-459e-8ea2-98c6458e0d08%2FUntitled.png?width=707.9750366210938&amp;table=block&amp;id=10e76526-d1cc-807a-9135-c365d0f06035" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc8081b96bc7a1c6eb890d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h2 id="https://www.notion.so/10e76526d1cc80bd9d2eea86b1c6c679" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc80bd9d2eea86b1c6c679"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">状态空间模型(SSM)</strong></span></span></h2><pre id="https://www.notion.so/10e76526d1cc80378bd4d0c60a630f87" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span> <span class="token keyword">def</span> <span class="token function">ssm</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Runs the SSM. See:
            - Algorithm 2 in Section 3.2 in the Mamba paper [1]
            - run_SSM(A, B, C, u) in The Annotated S4 [2]

        Args:
            x: shape (b, l, d_in)
        Returns:
            output: shape (b, l, d_in)

        Official Implementation:
            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311
            
        """</span>
        <span class="token punctuation">(</span>d_in<span class="token punctuation">,</span> n<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>A_log<span class="token punctuation">.</span>shape

        <span class="token comment"># Compute ∆ A B C D, the state space parameters.</span>
        <span class="token comment">#     A, D 是独立于输入的 (see Mamba paper [1] Section 3.5.2 "Interpretation of A" for why A isn't selective)</span>
        <span class="token comment">#     ∆, B, C 是依赖于输入的 (这是Mamba模型和 linear time invariant S4 的主要区别,这也是为什么Mamba被称为selective state spaces</span>
        
        A <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>A_log<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># shape (d_in, n)</span>
        D <span class="token operator">=</span> self<span class="token punctuation">.</span>D<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># (d_in,)</span>

        x_dbl <span class="token operator">=</span> self<span class="token punctuation">.</span>x_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># (b, l, dt_rank + 2*n)   </span>
        
        <span class="token punctuation">(</span>delta<span class="token punctuation">,</span> B<span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token operator">=</span> x_dbl<span class="token punctuation">.</span>split<span class="token punctuation">(</span>split_size<span class="token operator">=</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>args<span class="token punctuation">.</span>dt_rank<span class="token punctuation">,</span> n<span class="token punctuation">,</span> n<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># delta: (b, l, dt_rank) </span>
        <span class="token comment"># B, C: (b, l, n)  </span>
        delta <span class="token operator">=</span> F<span class="token punctuation">.</span>softplus<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dt_proj<span class="token punctuation">(</span>delta<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># (b, l, d_in)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>selective_scan<span class="token punctuation">(</span>x<span class="token punctuation">,</span> delta<span class="token punctuation">,</span> A<span class="token punctuation">,</span> B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> D<span class="token punctuation">)</span>  <span class="token comment"># 选择性扫描算法</span>
        <span class="token keyword">return</span> y
</span></span></span></code></pre><div id="https://www.notion.so/10e76526d1cc800397ecd4483de3c612" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Δ 在SSM中的作用，类似于RNN中的门控机制。</span></span></p></div><h2 id="https://www.notion.so/10e76526d1cc80a9b60ac5404c994c8c" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc80a9b60ac5404c994c8c"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">选择性扫描算法（selective_scan）</strong></span></span></h2><ol class="NumberedListWrapper"><li id="https://www.notion.so/10e76526d1cc80f097b2fd6ba9deb4cf" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">离散化的作用</strong></span><span class="SemanticString">：</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/1afa14243d5047ef8d0a320360e2bdb9" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">模型适应性：</strong></span><span class="SemanticString"> 离散化可以使得连续时间下的状态空间模型适应于计算机处理，因为计算机是离散的系统，需要将连续时间的问题转换为离散时间的问题。</span></span></li><li id="https://www.notion.so/dc58248b928742a79c5ee54ff846bcb6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">数值稳定性：</strong></span><span class="SemanticString"> 离散化可以提高数值计算的稳定性和精度，避免了在连续时间下可能出现的数值不稳定性问题。</span></span></li><li id="https://www.notion.so/b43e0825a3b94d1cb3a7fe419cc58ea2" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">计算效率：</strong></span><span class="SemanticString"> 离散化后的模型可以利用计算机的并行计算能力和优化算法进行高效计算。</span></span></li></ul></li><li id="https://www.notion.so/10e76526d1cc800d9f8dce0ff9491d60" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">选择性扫描算法原理</strong></span><span class="SemanticString">：</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/6d8e3ed40ff84ec2b72175a421c6d211" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">离散化处理：</strong></span><span class="SemanticString"> 首先对连续参数进行离散化处理，包括状态转移矩阵A和控制输入矩阵B。这里采用了零阶保持（ZOH）离散化和简化的欧拉离散化方法。</span></span></li><li id="https://www.notion.so/e37d92193f234618957c92b0856386ba" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">状态转移计算：</strong></span><span class="SemanticString"> 根据离散化后的参数计算状态转移矩阵deltaA和控制输入矩阵deltaB_u，然后通过选择性扫描算法逐步计算状态转移后的状态x。</span></span></li><li id="https://www.notion.so/e5fbf6bfe40e4ce8a48cee5957bb30c6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">输出计算：</strong></span><span class="SemanticString"> 在每一步状态转移后，根据状态转移后的状态x和输出矩阵C计算输出y，并将每一步计算的输出y按序列长度维度进行堆叠，得到最终的输出结果。</span></span></li></ul></li></ol><div id="https://www.notion.so/10e76526d1cc80999d24cad783f42838" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">第5步是对连续的矩阵A,B进行离散化得到离散化后的矩阵A和B, 离散化的方法有欧拉方法和零阶保持（Zero-order hold, ZOH）方法。下面的公式是ZOH算法
</span></span></p></div><div id="https://www.notion.so/10e76526d1cc805b9f1bdcfdd343cfc8" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F4998bcc2-9cde-4140-a1e6-ba34275be5b3%2FUntitled.png?width=342&amp;table=block&amp;id=10e76526-d1cc-805b-9f1b-dcfdd343cfc8"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F4998bcc2-9cde-4140-a1e6-ba34275be5b3%2FUntitled.png?width=342&amp;table=block&amp;id=10e76526-d1cc-805b-9f1b-dcfdd343cfc8" style="width:342px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc80469459eb7afc4f5451" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">第6步是对离散化后的矩阵A和B以及C执行SSM算法，其中离散的SSM方程如下
</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8026a9ebe612b791b41e" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fbebdb450-5e41-4408-80c0-08d967cf78b4%2FUntitled.png?width=278&amp;table=block&amp;id=10e76526-d1cc-8026-a9eb-e612b791b41e"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fbebdb450-5e41-4408-80c0-08d967cf78b4%2FUntitled.png?width=278&amp;table=block&amp;id=10e76526-d1cc-8026-a9eb-e612b791b41e" style="width:278px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc80e1badbc3978e8678ff" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">其中x表示的隐藏状态，u表示的输入，y表示的是输出。
</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8033aa4ce02dd88b4d2c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">代码中采用ZOH对矩阵A进行离散化，但是作者并没有采用ZOH对B进行离散化，而是采用了一种更简化的方式（因为主要的参数是A, 对B进行简化并不会影响实验的性能）</span></span></p></div><div id="https://www.notion.so/10e76526d1cc802399c1d7686ea43378" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在第六步中，代码中使用for循环的方式执行SSM, 主要是为了说明SSM的核心功能，其并行扫描算法</span></span></p></div><pre id="https://www.notion.so/10e76526d1cc80b08ce9f93e9f86e0a7" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span>    <span class="token keyword">def</span> <span class="token function">selective_scan</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> u<span class="token punctuation">,</span> delta<span class="token punctuation">,</span> A<span class="token punctuation">,</span> B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> D<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''    
        Args:
            u: shape (b, l, d_in)    输入x，(B,L,D)
            delta: shape (b, l, d_in)  离散步长,(B,L,D)
            A: shape (d_in, n)  连续的矩阵A,(D,N)
            B: shape (b, l, n)  连续的矩阵B(B,L,N)
            C: shape (b, l, n)  (B,L,N)
            D: shape (d_in,)    (D,)
    
        Returns:
            output: shape (b, l, d_in)   输出：(B,L,D)
    
        官方实现版本:
            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86
            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.
       '''</span>      
      
        <span class="token punctuation">(</span>b<span class="token punctuation">,</span> l<span class="token punctuation">,</span> d_in<span class="token punctuation">)</span> <span class="token operator">=</span> u<span class="token punctuation">.</span>shape <span class="token comment">#(B,L,D)</span>
        n <span class="token operator">=</span> A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment"># N</span>
        
        <span class="token triple-quoted-string string">'''
        对连续的参数(A, B)进行离散化
        A 使用零阶保持法(zero-order hold, ZOH)进行离散化 (see Section 2 Equation 4 in the Mamba paper [1])
        B 则使用一种简化的Euler方法进行离散化
        B没有使用ZOH的原因，作者解释如下: "A is the more important term and the performance doesn't change much with the simplification on B"
        '''</span>
        <span class="token comment"># einsum 操作实际上是将 delta 和 A 张量的最后两个维度进行矩阵乘法，并在前面添加了两个维度（b 和 l）</span>
        <span class="token comment"># torch.exp 函数对这个张量中的每个元素进行指数化，即将每个元素取指数值。</span>
        deltaA <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>einsum<span class="token punctuation">(</span>delta<span class="token punctuation">,</span> A<span class="token punctuation">,</span> <span class="token string">'b l d_in, d_in n -> b l d_in n'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment"># (B,L,D) * (D,N) -> (B,L,D,N)</span>
        <span class="token comment"># 将 delta、B 和 u 张量的对应位置元素相乘，并在最后一个维度上进行求和，输出一个新的张量。</span>
        deltaB_u <span class="token operator">=</span> einsum<span class="token punctuation">(</span>delta<span class="token punctuation">,</span> B<span class="token punctuation">,</span> u<span class="token punctuation">,</span> <span class="token string">'b l d_in, b l n, b l d_in -> b l d_in n'</span><span class="token punctuation">)</span>  <span class="token comment"># (B,L,D)*(B,L,N)*(B,L,D)->(B,L,D,N)</span>
        
        <span class="token triple-quoted-string string">'''
        执行 selective scan (see scan_SSM() in The Annotated S4 [2])
        # 注意，下面的代码是顺序执行的, 然而在官方代码中使用更快的并行扫描算法实现的(类似于FlashAttention，采用硬件感知扫描)。
        '''</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> d_in<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>deltaA<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        ys <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 这里使用for循环的方式只用来说明核心的逻辑，原代码中采用并行扫描算法</span>
            x <span class="token operator">=</span> deltaA<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*</span> x <span class="token operator">+</span> deltaB_u<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token comment"># x(t + 1) = Ax(t) + Bu(t)</span>
            y <span class="token operator">=</span> einsum<span class="token punctuation">(</span>x<span class="token punctuation">,</span> C<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'b d_in n, b n -> b d_in'</span><span class="token punctuation">)</span> <span class="token comment"># y(t) = Cx(t)  (B,D,N)*(B,N)->(B,D) </span>
            ys<span class="token punctuation">.</span>append<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        y <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>ys<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 大小 (b, l, d_in)  (B,L,D)</span>
        y <span class="token operator">=</span> y <span class="token operator">+</span> u <span class="token operator">*</span> D <span class="token comment"># y(t) = Cx(t)+Du(t)</span>
        <span class="token keyword">return</span> y <span class="token comment">#(B,L,D)</span>
</span></span></span></code></pre><div id="https://www.notion.so/10e76526d1cc8062bba1de8a523016ca" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/10e76526d1cc804f917ac6f4b7f28566" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc804f917ac6f4b7f28566"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">视觉mamba</strong></span></span></h1><div id="https://www.notion.so/10e76526d1cc802a8fcacac3c6d624c3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">关于 ViT，Transformers 很有效，但通常需要大量的计算能力，尤其是对于高质量图像。Vision Mamba (ViM) 通过提供更高效的选项解决了这个问题。</span></span></p></div><h2 id="https://www.notion.so/10e76526d1cc80309d5bc9936fd41acc" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc80309d5bc9936fd41acc"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Vision Mamba vs Transformers</strong></span></span></h2><div id="https://www.notion.so/10e76526d1cc801a889df7da2bc92edc" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F0a12d42c-f1a8-49ab-889a-a07ffa5d0254%2Fimage.png?width=707.9874877929688&amp;table=block&amp;id=10e76526-d1cc-801a-889d-f7da2bc92edc"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F0a12d42c-f1a8-49ab-889a-a07ffa5d0254%2Fimage.png?width=707.9874877929688&amp;table=block&amp;id=10e76526-d1cc-801a-889d-f7da2bc92edc" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc803da2eaf2fa1bf7054b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">论文详细分析了Mamba如何处理视觉任务，该模型之所以高效是因为它使用了双向状态空间模型，比一般的Transformer模型能更快地处理图像数据。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc805796c8ebf5c6f9535b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">处理图像比处理文本更棘手。图像不仅仅是像素序列；它们具有复杂的图案、不同的空间关系，并且需要了解整个场景。这使得高效处理视觉数据变得困难，尤其是在处理大量数据或高分辨率图像时。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80a3823ed077195b0b09" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F5b9210a6-f112-409e-b46c-9dad444d5929%2Fimage.png?width=707.9874877929688&amp;table=block&amp;id=10e76526-d1cc-80a3-823e-d077195b0b09"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F5b9210a6-f112-409e-b46c-9dad444d5929%2Fimage.png?width=707.9874877929688&amp;table=block&amp;id=10e76526-d1cc-80a3-823e-d077195b0b09" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc80b69567f5c9e60c6022" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">此外，Mamba 块是 ViM 的重要组成部分，它使用位置嵌入来标记图像中的序列。通过应用双向状态空间模型来简化视觉表示，Vision Mamba 可以成功掌握图像的整体背景。该方法解决了视觉数据对位置的自然敏感性，这是传统 Transformer 模型中常见的问题，尤其是在高分辨率图像中。</span></span></p></div><h2 id="https://www.notion.so/10e76526d1cc803baaa2d64e8b31f997" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc803baaa2d64e8b31f997"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Vision Mamba编码器</span></span></h2><div id="https://www.notion.so/10e76526d1cc808a977de4d4dd45626b" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F1f7c4150-96f9-4c7a-9012-c0d1044bc751%2Fimage.png?width=707.9750366210938&amp;table=block&amp;id=10e76526-d1cc-808a-977d-e4d4dd45626b"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F1f7c4150-96f9-4c7a-9012-c0d1044bc751%2Fimage.png?width=707.9750366210938&amp;table=block&amp;id=10e76526-d1cc-808a-977d-e4d4dd45626b" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc80bc9408cdf8fc123924" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">ViM 模型首先将输入图像分成小块，然后将小块投影成 token。然后这些 token 被输入到 ViM 编码器中。然后序列中的每个 token 经历两个单独的线性变换。该算法在正向和反向两个方向上处理这些转换后的 token，模拟双向神经网络层。与用于建模文本序列的 Mamba 模型不同，ViM 编码器在正向和反向两个方向上处理 token 序列。对于每个方向，该过程涉及应用一维卷积，然后应用 Sigmoid 线性单元 (SiLU) 激活函数。对于 ImageNet 分类等任务，会在 token 序列中添加一个额外的可学习分类 token（BERT 开始一致使用此 token）。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80c1bf43e1d5c29063fd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">ViM 的突出特点之一是其双向处理能力，类似于 LSTM 的工作方式。与许多以单向方式处理数据的模型不同，ViM 的编码器可以正向和反向处理标记。双向模型可以更深入地理解图像上下文，这是准确进行图像分类和分割的关键因素。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc807195b7dad81d9c890a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">一旦对 token 进行卷积和激活，算法就会执行额外的线性变换并应用 softplus 函数，以确保输出值保持正值。这些变换为 SSM 序列建模函数准备 token。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc80859008c92fac535dd4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在 SSM 操作之后，该算法应用了一种门控机制，通过将 SSM 输出与 SiLU 激活的前向和后向序列逐元素相乘来调节信息流。这种门控机制可能旨在控制每个方向处理的贡献。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8027b194dc369750916d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">最后一步是残差连接，将原始输入序列添加到门控输出，这有助于保留来自早期层的信息并解决梯度消失问题。整个过程的输出是一个新的标记序列，该序列可能经过了复杂的转换，捕获了序列两个方向上的复杂依赖关系。算法最后返回这个转换后的标记序列。</span></span></p></div><h2 id="https://www.notion.so/10e76526d1cc805b9fbfe289b58d7364" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc805b9fbfe289b58d7364"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">高分辨率图像处理</strong></span></span></h2><div id="https://www.notion.so/10e76526d1cc80138fb5ff4ca5738346" class="Image Image--PageWidth"><figure><a href="#?width=700"><img src="#?width=700" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/10e76526d1cc806f9495d59921b23870" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">论文强调了在不同领域处理高分辨率图像的重要性。例如，在卫星图像中，高分辨率对于详细检查和准确发现至关重要。同样，在 PCB 制造等工业环境中，检测高分辨率图像中的小缺陷对于保持质量至关重要。事实证明，ViM 在管理此类任务方面非常有效。</span></span></p></div><h2 id="https://www.notion.so/10e76526d1cc80da8a1ae43f53db861d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc80da8a1ae43f53db861d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Vision mamba简单复现</span></span></h2><div id="https://www.notion.so/10e76526d1cc80dd9966f2a1e9235163" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/10e76526d1cc802096f4f35ec89b4702" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h2 id="https://www.notion.so/10e76526d1cc80d2a592d36f6a1a2795" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/10e76526d1cc80d2a592d36f6a1a2795"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">结论</strong></span></span></h2><div id="https://www.notion.so/10e76526d1cc80109acbf6ca933cd852" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">本文介绍了一种将 Mamba 应用于视觉任务的技术，即使用双向状态空间模型 (SSM) 进行全局视觉上下文建模和位置嵌入。这种方法表明，传统的注意力机制可能会过时，因为 ViM 可以有效地捕捉视觉数据的位置上下文，而无需依赖基于转换器的注意力机制。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8085aa9df61e6d40414f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">ViM 以其亚二次计算时间和线性内存复杂度脱颖而出，与 Transformer 模型中常见的二次增长形成鲜明对比。这种效率使 ViM 特别擅长处理高分辨率图像。</span></span></p></div><div id="https://www.notion.so/10e76526d1cc8028bb90cfa2b18cbafb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在 ImageNet 分类等基准上进行的大量测试证实了 ViM 的性能和效率，展示了其作为计算机视觉领域强大工具的潜力。</span></span></p></div></article>
  <footer class="Footer">
  <div>&copy; Simon’s Blogs 2022</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>