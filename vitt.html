<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="https://avatars.githubusercontent.com/u/55181594?v=4">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>Vision-Transformer 研习&nbsp;|&nbsp;Simon’s Blogs</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="Vision-Transformer 研习">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😶‍🌫️&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="https://avatars.githubusercontent.com/u/55181594?v=4"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😀&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About me</span>
        </div>
      </a>
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😶‍🌫️&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">Vision-Transformer 研习</h1>
    
  </header>
  <article id="https://www.notion.so/921945c25fe64f2e92beafdb77ec782b" class="PageRoot"><h2 id="https://www.notion.so/e192411043d84ca9a63b50ff373db0fc" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/e192411043d84ca9a63b50ff373db0fc"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">所有attention放vit_attention文件夹下，及插及用。</span></span></h2><h3 id="https://www.notion.so/b18a6284ffc74b09872280609c00a96d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/b18a6284ffc74b09872280609c00a96d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">已经实现</span></span></h3><div id="https://www.notion.so/ef6e6adf747a44da868ddc632a591ce1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/Jacky-Android/Study-Vision-transformer/blob/main/vit_attention/Vit.py">vsion transformer attention</a></span></span></p></div><div id="https://www.notion.so/d211201cc5a543f09aac9feadfee53c6" class="Bookmark"><a href="https://github.com/Jacky-Android/Study-Vision-transformer/blob/main/vit_attention/swin_att.py"><p class="Bookmark__Link">https://github.com/Jacky-Android/Study-Vision-transformer/blob/main/vit_attention/swin_att.py</p></a></div><h1 id="https://www.notion.so/fe6fce469b904bf681df7abc2994f746" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/fe6fce469b904bf681df7abc2994f746"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Q,K,V</span></span></h1><div id="https://www.notion.so/ac03f3b870c145f48b0dfd4739b79151" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/09e0238f884045d096047b6c649e8e7a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Transformer里面的Q，K，V是指查询（Query），键（Key）和值（Value）三个矩阵，它们都是通过对输入进行线性变换得到的。它们的作用是实现一种注意力机制（Attention），用于计算输入的每个元素（token）之间的相关性，并根据相关性对输入进行加权和，得到一个新的输出。</span></span></p></div><div id="https://www.notion.so/8e7aa56e86b348889041a89198e2986d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">具体来说，查询矩阵Q用于询问键矩阵K中的哪个token与查询最相似，通过点积计算得到一个相似度序列。然后对相似度序列进行归一化处理，得到一个概率分布，表示每个token被注意的程度。最后，用这个概率分布对值矩阵V进行加权和，得到一个新的token，表示查询的结果。</span></span></p></div><div id="https://www.notion.so/283dc6f106e1438a8a1246992ac4d7d4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这种注意力机制可以让模型学习到输入的每个元素之间的依赖关系，从而提高模型的表达能力和性能。Transformer模型中使用了两种不同的注意力机制，分别是自注意力（Self-Attention）和编码器-解码器注意力（Encoder-Decoder Attention）。</span></span></p></div><div id="https://www.notion.so/53948fe2763c42b4bc84412bd8664ff2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">自注意力是指Q，K，V都来自于同一个输入，用于计算输入的每个元素与自身的相关性，从而捕捉输入的内部结构。编码器-解码器注意力是指Q来自于解码器的输出，K，V来自于编码器的输出，用于计算解码器的输出与编码器的输出的相关性，从而捕捉输入和输出之间的对应关系。</span></span></p></div><div id="https://www.notion.so/bebb5e602e19406b8406ca9cc3b4e513" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F0a7613a6-fe60-4bc6-a93e-9a264e1dc794%2FUntitled.png?width=271&amp;table=block&amp;id=bebb5e60-2e19-406b-8406-ca9cc3b4e513"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F0a7613a6-fe60-4bc6-a93e-9a264e1dc794%2FUntitled.png?width=271&amp;table=block&amp;id=bebb5e60-2e19-406b-8406-ca9cc3b4e513" style="width:271px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h2 id="https://www.notion.so/02ddcb2eec9241369f34a3089a4d7b25" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/02ddcb2eec9241369f34a3089a4d7b25"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">QKV Attention</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/93c60eb8527b40158871e7c2c8c9c913" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">首先，将输入的特征向量（例如词向量）分别乘以三个不同的可学习的权重矩阵，得到查询矩阵（query matrix）Q，键矩阵（key matrix）K和值矩阵（value matrix）V。这相当于对输入的特征向量进行了三种不同的线性变换，得到了三种不同的表示。</span></span></li><li id="https://www.notion.so/306bb96245304071bf33ddfb74a83765" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">然后，计算Q和K的点积，得到注意力的对数值（logit），并乘以一个缩放因子（scale），用于调节注意力的权重。缩放因子通常是K的维度的平方根的倒数，用于避免点积值过大或过小，影响梯度的稳定性。</span></span></li><li id="https://www.notion.so/3a0e7e7db448486ea7c97a7e57147f11" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">接着，对注意力的对数值进行softmax激活，得到注意力的权重（weight），表示每个输入元素被注意的程度。注意力的权重是一个概率分布，它的和为1。</span></span></li><li id="https://www.notion.so/55856577b4a44a3d95af25a0d3f5acd9" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">最后，用注意力的权重对V进行加权和，得到注意力的输出（output），表示每个输入元素的新的表示。注意力的输出是一个加权平均的结果，它的维度和V相同。</span></span></li></ul><div id="https://www.notion.so/cd0441975f604f599af87673ba776c15" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">QKV attention 计算公式可以用数学公式表示为：</span></span></p></div><p id="https://www.notion.so/0e58a11a0c064dfea3b1a7a5437663db" class="Equation" data-latex="Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">A</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.448331em;vertical-align:-0.93em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183309999999999em;"><span style="top:-2.25278em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85722em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.18278000000000005em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span></span></p><div id="https://www.notion.so/a90ff1f78b9d4fc1a9e29d1fb73b8681" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/92bfec638ae44f118b3bcf67bd43fe98" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">其中，dk是K的维度，QKT表示Q和K的转置的点积，softmax函数将注意力的对数值归一化，使得它们的和为1。</span></span></p></div><div id="https://www.notion.so/2f10f0ed996d4ccf9100052d0ea165f5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/7d573a49793e4598bf72fcdae06f100b" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/7d573a49793e4598bf72fcdae06f100b"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Say something</span></span></h1><div id="https://www.notion.so/51d927f66a684f29adafd4d262787f45" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">实现一些经典的vit attention ，也算是笔记。😁😁😁</span></span></p></div><div id="https://www.notion.so/b9b35c308ad54f2fae7881cf35797f72" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/4c5a92de07d4459da62d7b62e94ba93f" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/4c5a92de07d4459da62d7b62e94ba93f"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Vision transformer</span></span></h1><div id="https://www.notion.so/fbdb125c53ce43feb3618c773cf3dec1" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa6c3faec-6701-4d8a-a196-a4a79ed0a739%2FUntitled.png?width=711&amp;table=block&amp;id=fbdb125c-53ce-43fe-b361-8c773cf3dec1"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa6c3faec-6701-4d8a-a196-a4a79ed0a739%2FUntitled.png?width=711&amp;table=block&amp;id=fbdb125c-53ce-43fe-b361-8c773cf3dec1" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/f9883e3f7e31430cb9ef179decea73c3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">1) patch embedding：例如输入图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，则每张图像会生成224x224/16x16=196个patch，即输入序列长度为</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">196</strong></span><span class="SemanticString">，每个patch维度16x16x3=</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">768</strong></span><span class="SemanticString">，线性投射层的维度为768xN (N=768)，因此输入通过线性投射层之后的维度依然为196x768，即一共有196个token，每个token的维度是768。这里还需要加上一个特殊字符cls，因此最终的维度是</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">197x768</strong></span><span class="SemanticString">。到目前为止，已经通过patch embedding将一个视觉问题转化为了一个seq2seq问题</span></span></p></div><div id="https://www.notion.so/97f1be0e712b41dfb324cc7ae9d04591" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">(2) positional encoding（standard learnable 1D position embeddings）：ViT同样需要加入位置编码，位置编码可以理解为一张表，表一共有N行，N的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列embedding的维度相同（768）。注意位置编码的操作是sum，而不是concat。加入位置编码信息之后，维度依然是</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">197x768</strong></span></span></p></div><div id="https://www.notion.so/2b28e35787ce4811a41019834643df45" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">(3) LN/multi-head attention/LN：LN输出维度依然是197x768。多头自注意力时，先将输入映射到q，k，v，如果只有一个头，qkv的维度都是197x768，如果有12个头（768/12=64），则qkv的维度是197x64，一共有12组qkv，最后再将12组qkv的输出拼接起来，输出维度是197x768，然后在过一层LN，维度依然是</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">197x768</strong></span></span></p></div><div id="https://www.notion.so/325aab2c65c34057955561f1e815600e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">(4) MLP：将维度放大再缩小回去，197x768放大为197x3072，再缩小变为</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">197x768</strong></span></span></p></div><h2 id="https://www.notion.so/fd85f44b9083440e8d9abb7a2b435c47" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/fd85f44b9083440e8d9abb7a2b435c47"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">vision transformer的attention实现</span></span></h2><pre id="https://www.notion.so/2a413fcc25ef4c11a225d008889083d1" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 dim<span class="token punctuation">,</span>   <span class="token comment"># 输入token的dim</span>
                 num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
                 qkv_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                 qk_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 attn_drop_ratio<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span>
                 proj_drop_ratio<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Attention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        head_dim <span class="token operator">=</span> dim <span class="token operator">//</span> num_heads
        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> qk_scale <span class="token keyword">or</span> head_dim <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>
        self<span class="token punctuation">.</span>qkv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>qkv_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>attn_drop_ratio<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>proj_drop_ratio<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># [batch_size, num_patches + 1, total_embed_dim]</span>
        B<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape

        <span class="token comment"># qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]</span>
        <span class="token comment"># reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]</span>
        <span class="token comment"># permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span>
        qkv <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> N<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> C <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
        <span class="token comment"># [batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span>
        q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v <span class="token operator">=</span> qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>  <span class="token comment"># make torchscript happy (cannot use tensor as tuple)</span>

        <span class="token comment"># transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]</span>
        <span class="token comment"># @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]</span>
        attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale
        attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        attn <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_drop<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>

        <span class="token comment"># @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span>
        <span class="token comment"># transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]</span>
        <span class="token comment"># reshape: -> [batch_size, num_patches + 1, total_embed_dim]</span>
        x <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x</span></span></span></code></pre><h1 id="https://www.notion.so/4b67c5f6111a4e47abf6ee21d16fd1ff" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/4b67c5f6111a4e47abf6ee21d16fd1ff"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Swin Tranformer</span></span></h1><h2 id="https://www.notion.so/6e0661914a0445b7bc8ccd4a8e361de7" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/6e0661914a0445b7bc8ccd4a8e361de7"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">框架</span></span></h2><div id="https://www.notion.so/8235e1536b494bb5bdcfbfd845b199b4" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F43e90388-9d71-4270-bca1-b90b4dc217d9%2FUntitled.png?width=1302&amp;table=block&amp;id=8235e153-6b49-4bb5-bdcf-bfd845b199b4"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F43e90388-9d71-4270-bca1-b90b4dc217d9%2FUntitled.png?width=1302&amp;table=block&amp;id=8235e153-6b49-4bb5-bdcf-bfd845b199b4" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/e1198a576ddc481fa0824197d59d69ae" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Swin Transformer（上图为 Swin-T，T 为 Tiny）首先通过补丁分割模块（如</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://sh-tsang.medium.com/review-vision-transformer-vit-406568603de0">ViT）</a></span><span class="SemanticString">将输入 RGB 图像分割为不重叠的补丁。</span></span></li><li id="https://www.notion.so/7a481d78a4674382ac4708b8b8147319" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">每个补丁都被视为一个“令牌”，其特征被设置为原始像素 RGB 值的串联。使用</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">4×4 的 patch 大小，因此每个 patch 的特征维度为 4×4×3=48</strong></span><span class="SemanticString">。线性嵌入层应用于该原始值特征，将</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">其投影到任意维度</strong></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">C</strong></em></span><span class="SemanticString">。</span></span></li><li id="https://www.notion.so/0733d2be96f349eeb7894de850ea21fd" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">为了产生</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">分层表示</strong></span><span class="SemanticString">，随着网络变得更深，通过补丁合并层来减少标记的数量。第一个补丁合并层</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">连接每组 2×2 相邻补丁的特征，并在4 </strong></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">C</strong></em></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">维连接特征</strong></span><span class="SemanticString">上应用线性层。这</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">将令牌数量减少了 2×2 = 4 的倍数</strong></span><span class="SemanticString">（2 次分辨率下采样）</span></span></li><li id="https://www.notion.so/283149b2fbd4433ebbc3d07f2fb38215" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">输出</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">尺寸</strong></span><span class="SemanticString">设置为</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">2 </strong></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">C</strong></em></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">，</em></span><span class="SemanticString">分辨率保持为</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">H</strong></em></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"> /8× </strong></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">W</strong></em></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"> /8</strong></span><span class="SemanticString">。补丁合并和特征转换的第一个块被表示为</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">“阶段 2”</strong></span><span class="SemanticString">。</span></span></li><li id="https://www.notion.so/e9a7da08af854368a916477814194168" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">在每个 MSA 模块和每个 MLP 之前应用 LayerNorm (LN) 层，并在</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">每个</strong></span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://sh-tsang.medium.com/review-layer-normalization-ln-6c2ae88bae47"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">模块</strong></a></span><span class="SemanticString">之后应用</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">残差连接。</strong></span></span></li></ul><h2 id="https://www.notion.so/433f700aa1ff42dd8a8bf04868d0c672" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/433f700aa1ff42dd8a8bf04868d0c672"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Shifted Window Based Self-Attention</strong></strong></span></span></h2><h3 id="https://www.notion.so/53f808744c4f4c25981930c706510c1a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/53f808744c4f4c25981930c706510c1a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Window Based Self-Attention (W-MSA)</strong></strong></span></span></h3><div id="https://www.notion.so/5f8e0de73862436ab33c839e514b7d05" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F3f0b06b0-513c-4de2-b388-f7e2f8642e5b%2FUntitled.png?width=462&amp;table=block&amp;id=5f8e0de7-3862-436a-b33c-839e514b7d05"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F3f0b06b0-513c-4de2-b388-f7e2f8642e5b%2FUntitled.png?width=462&amp;table=block&amp;id=5f8e0de7-3862-436a-b33c-839e514b7d05" style="width:462px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/26891c948e1e485ba798353c2081e64e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">假设每个窗口包含</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">M</strong></em></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"> × </strong></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">M 个</strong></em></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">patch</strong></span><span class="SemanticString">，全局 MSA 模块和基于</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">h</strong></em></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"> × </strong></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">w</strong></em></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">个patch图像</strong></span><span class="SemanticString">的窗口的计算复杂度为：</span></span></p></div><div id="https://www.notion.so/e6b584dbf0194234b6c066c46448cc30" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F7ec378fa-a36f-44e3-809f-a1efc460ef6f%2FUntitled.png?width=390&amp;table=block&amp;id=e6b584db-f019-4234-b6c0-66c46448cc30"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F7ec378fa-a36f-44e3-809f-a1efc460ef6f%2FUntitled.png?width=390&amp;table=block&amp;id=e6b584db-f019-4234-b6c0-66c46448cc30" style="width:390px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/df7e01ac79e44ca28d56739168ba1ea6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">其中前者与补丁号</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">hw</em></span><span class="SemanticString">成二次方，后者</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">在</strong></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">M</strong></em></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">固定（默认设置为 7）</strong></span><span class="SemanticString">时呈线性。</span></span></p></div><h3 id="https://www.notion.so/7bb082527626433f8c0685895c78a780" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/7bb082527626433f8c0685895c78a780"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Window Based Self-Attention (W-MSA)</strong></strong></span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/ba109964c0a04e82a734a391c80e061e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">基于窗口的自注意力模块</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">缺乏跨窗口的连接</strong></span><span class="SemanticString">，这限制了它的建模能力。</span></span></li><li id="https://www.notion.so/3860f1dce6e943d49f90e0ffb170104c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">提出了一种移位窗口分区方法，该方法</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">在连续 Swin Transformer 块中的两个分区配置之间交替</strong></span><span class="SemanticString">。</span></span></li></ul><div id="https://www.notion.so/a559fa877d3c4496a692c41f049d49e2" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F65121772-9696-4830-8a42-72a872b0cbcd%2FUntitled.png?width=448&amp;table=block&amp;id=a559fa87-7d3c-4496-a692-c41f049d49e2"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F65121772-9696-4830-8a42-72a872b0cbcd%2FUntitled.png?width=448&amp;table=block&amp;id=a559fa87-7d3c-4496-a692-c41f049d49e2" style="width:448px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/ec876677675c4ed69fa96b356db15878" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">• 其中</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">zl</em></span><span class="SemanticString"> -1 是前一层的输出特征。</span></span></p></div><div id="https://www.notion.so/14ec823d54484203866b0f789e064ac0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"> 在计算相似性时，每个头都包含</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">相对位置偏差</strong></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">B。</strong></em></span></span></p></div><div id="https://www.notion.so/efc3fc59498947768829e0e6a9e8a173" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F956e0376-aff5-4f8e-964c-67284a7ecef5%2FUntitled.png?width=552&amp;table=block&amp;id=efc3fc59-4989-4776-8829-e0e6a9e8a173"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F956e0376-aff5-4f8e-964c-67284a7ecef5%2FUntitled.png?width=552&amp;table=block&amp;id=efc3fc59-4989-4776-8829-e0e6a9e8a173" style="width:552px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h2 id="https://www.notion.so/673128ce56db4a82914332a4fda4c416" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/673128ce56db4a82914332a4fda4c416"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">细节</span></span></h2><h3 id="https://www.notion.so/09214b31dc0f43138b825b41749fec70" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/09214b31dc0f43138b825b41749fec70"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">window_partition</span></span></h3><div id="https://www.notion.so/964c359a651048e4ac8c08539fcbbd36" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我复现的代码如下</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/e51302bd49e4445f99032f530da610a4" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">首先，将输入的图像张量x（B, C,H, W）按照窗口大小window_size划分为（B, C,H // window_size, window_size, W // window_size, window_size）的形状，其中B是批量大小，H是图像高度，W是图像宽度，C是通道数。</span></span></li><li id="https://www.notion.so/e044ab9948a443c784aacb36e3ac4a7a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">然后，将x在第二维和第四维上进行交换，得到（B, C, W // window_size, H // window_size, window_size, window_size）的形状，这样可以保证每个窗口内部的元素是连续存储的。</span></span></li><li id="https://www.notion.so/d68dc7071575489d8446be922e4955d4" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">最后，将x展平为（num_windows * B, window_size, window_size, C）的形状，其中</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">num_windows = (H // window_size) * (W // window_size)</strong></em></span><span class="SemanticString">是每张图像划分出的窗口数量。</span></span></li></ul><div id="https://www.notion.so/e86242c15eab4ec1bb2fd3a321d74bb4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">输入feature map为 [1,128,32,32] ，输出形状为 [16,64,128] 的tokens</span></span></p></div><pre id="https://www.notion.so/6867b140d9154129aad2b22a38ecc9e4" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">def</span> <span class="token function">img2windows</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> H_sp<span class="token punctuation">,</span> W_sp<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    img: B C H W
    """</span>
    B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> img<span class="token punctuation">.</span>shape
    img_reshape <span class="token operator">=</span> img<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H <span class="token operator">//</span> H_sp<span class="token punctuation">,</span> H_sp<span class="token punctuation">,</span> W <span class="token operator">//</span> W_sp<span class="token punctuation">,</span> W_sp<span class="token punctuation">)</span>
     <span class="token comment"># [batch_size*num_windows, Mh*Mw, total_embed_dim]</span>
    img_perm <span class="token operator">=</span> img_reshape<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> H_sp <span class="token operator">*</span> W_sp<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
    <span class="token keyword">return</span> img_perm
</span></span></span></code></pre><h3 id="https://www.notion.so/6f81302452d940f8bb6d953d20e6fb86" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/6f81302452d940f8bb6d953d20e6fb86"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">windows attention</span></span></h3><div id="https://www.notion.so/9031b8468ecd4e74a8634e60b0aa19e7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">参照官方代码：</span></span></p></div><div id="https://www.notion.so/c6121f09744e4b2585cc08aa8dbce262" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这段代码是定义一个基于窗口的多头自注意力模块的类，它可以实现shifted和non-shifted两种窗口划分方式，并且加入了相对位置偏置。它的主要功能和参数如下：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/15ec54c7f56a43c7a65b898ee71aea88" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">__init__</strong></code></span><span class="SemanticString">方法是初始化模块的参数，包括输入通道数dim，窗口大小window_size，注意力头数num_heads，以及一些可选的参数，如qkv_bias，qk_scale，attn_drop和proj_drop。这些参数分别控制了是否给查询、键、值添加可学习的偏置，查询和键的缩放因子，注意力权重的dropout比例，以及输出的dropout比例。此外，这个方法还定义了一个相对位置偏置表relative_position_bias_table，它是一个可学习的张量，用来存储每个窗口内部每对位置之间的相对位置偏置。它的形状是(2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)，表示每个头有(2 * window_size[0] - 1) * (2 * window_size[1] - 1)种不同的相对位置关系。这个方法还注册了一个缓冲区relative_position_index，它是一个固定的张量，用来记录每个窗口内部每对位置之间的相对位置索引。它的形状是window_size[0] * window_size[1], window_size[0] * window_size[1]），表示每个窗口有window_size[0] * window_size[1]个位置，每个位置与其他位置之间有一个相对位置索引，范围是0到(2 * window_size[0] - 1) * (2 * window_size[1] - 1) - 1。</span></span></li><li id="https://www.notion.so/b29eb561fa8e4f8bac011829e368c1dc" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">forward</strong></code></span><span class="SemanticString">方法是执行模块的前向计算，它接受两个参数：x和mask。x是输入特征，它的形状是(num_windows*B, N, C)，其中num_windows是每张图像划分出的窗口数量，B是批量大小，N是每个窗口内部的位置数量（等于window_size[0] * window_size[1]），C是通道数。mask是一个可选的参数，它是一个掩码张量，用来屏蔽一些不需要计算注意力的位置。它的形状是(num_windows, N, N)，其中num_windows和N与x相同。如果没有提供mask，则默认为None。这个方法的主要步骤如下：</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/ad41a5de4c6a447883099821c2e32561" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">首先，调用self.qkv(x)得到查询、键、值三个张量，并将它们重塑为(B_, N, 3, self.num_heads, C // self.num_heads)的形状，并在第一维和第三维上进行交换，得到(3, B_, self.num_heads, N, C // self.num_heads)的形状。然后将这个张量分解为q, k, v三个张量，分别表示查询、键、值。</span></span></li><li id="https://www.notion.so/0a594e20da194a86b9fd5abdeceb828b" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">然后，将q乘以self.scale得到缩放后的查询，并与k进行转置矩阵乘法得到注意力得分张量attn。它的形状是(B_, self.num_heads, N, N)，表示每个头每个窗口内部每对位置之间的注意力得分。</span></span></li><li id="https://www.notion.so/fcb3afb40da2468ea09b9ef5998d674e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">接着，根据self.relative_position_index从self.relative_position_bias_table中取出相应的相对位置偏置，并将其重塑为(self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)的形状，并在第一维和第三维上进行交换，得到(-1, self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1])的形状。然后将其在第零维上增加一个维度，并与attn相加得到加入了相对位置偏置的注意力得分张量attn。</span></span></li><li id="https://www.notion.so/fcadae19d0924bffa3dc55829debb716" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">然后，如果提供了mask，则将attn重塑为(B_ // nW, nW, self.num_heads, N, N)的形状，并在第一维和第二维上增加一个维度，与mask相加得到屏蔽了一些位置的注意力得分张量attn。然后将其重塑为(-1, self.num_heads, N, N)的形状。如果没有提供mask，则直接将attn通过self.softmax函数得到注意力权重张量attn，并对其进行self.attn_drop操作。</span></span></li><li id="https://www.notion.so/c0c2057b9ed1411b956a523c9a899f24" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">最后，将attn与v进行矩阵乘法得到输出特征x，并将其在第一维和第二维上进行交换，并重塑为(B_, N, C)的形状。然后通过self.proj(x)得到最终的输出特征x，并对其进行self.proj_drop操作。返回x作为模块的输出。</span></span></li></ul></li></ul><div id="https://www.notion.so/e30115a4c48b42adac2d46ce0640445b" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F4abc1b76-6cb5-46a7-b02d-146bf571ab9f%2FUntitled.png?width=720&amp;table=block&amp;id=e30115a4-c48b-42ad-ac2d-46ce0640445b"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F4abc1b76-6cb5-46a7-b02d-146bf571ab9f%2FUntitled.png?width=720&amp;table=block&amp;id=e30115a4-c48b-42ad-ac2d-46ce0640445b" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/12beaa7f00184255901ae5cd882b0e18" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fb5bbcca0-e9be-4a27-ba09-cc08c27abcfc%2FUntitled.png?width=720&amp;table=block&amp;id=12beaa7f-0018-4255-901a-e5cd882b0e18"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fb5bbcca0-e9be-4a27-ba09-cc08c27abcfc%2FUntitled.png?width=720&amp;table=block&amp;id=12beaa7f-0018-4255-901a-e5cd882b0e18" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><pre id="https://www.notion.so/036a4db46177433aa6b895e5435f9245" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">class</span> <span class="token class-name">WindowAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> qkv_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> qk_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> attn_drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> proj_drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
    self<span class="token punctuation">.</span>window_size <span class="token operator">=</span> window_size  <span class="token comment"># Wh, Ww</span>
    self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
    head_dim <span class="token operator">=</span> dim <span class="token operator">//</span> num_heads
    self<span class="token punctuation">.</span>scale <span class="token operator">=</span> qk_scale <span class="token keyword">or</span> head_dim <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>

    <span class="token comment"># define a parameter table of relative position bias</span>
    self<span class="token punctuation">.</span>relative_position_bias_table <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>
        torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 2*Wh-1 * 2*Ww-1, nH</span>

    <span class="token comment"># get pair-wise relative position index for each token inside the window</span>
    coords_h <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    coords_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    coords <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span><span class="token punctuation">[</span>coords_h<span class="token punctuation">,</span> coords_w<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 2, Wh, Ww</span>
    coords_flatten <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>coords<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 2, Wh*Ww</span>
    relative_coords <span class="token operator">=</span> coords_flatten<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">-</span> coords_flatten<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># 2, Wh*Ww, Wh*Ww</span>
    relative_coords <span class="token operator">=</span> relative_coords<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># Wh*Ww, Wh*Ww, 2</span>
    relative_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>  <span class="token comment"># shift to start from 0</span>
    relative_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>
    relative_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>
    relative_position_index <span class="token operator">=</span> relative_coords<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Wh*Ww, Wh*Ww</span>
    self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"relative_position_index"</span><span class="token punctuation">,</span> relative_position_index<span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>qkv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>qkv_bias<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>attn_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>attn_drop<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>proj_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>proj_drop<span class="token punctuation">)</span>

    trunc_normal_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relative_position_bias_table<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">.02</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Args:
        x: input features with shape of (num_windows*B, N, C)
        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
    """</span>
    B_<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
    qkv <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B_<span class="token punctuation">,</span> N<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> C <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
    q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v <span class="token operator">=</span> qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>  <span class="token comment"># make torchscript happy (cannot use tensor as tuple)</span>

    q <span class="token operator">=</span> q <span class="token operator">*</span> self<span class="token punctuation">.</span>scale
    attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    relative_position_bias <span class="token operator">=</span> self<span class="token punctuation">.</span>relative_position_bias_table<span class="token punctuation">[</span>self<span class="token punctuation">.</span>relative_position_index<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>
        self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Wh*Ww,Wh*Ww,nH</span>
    relative_position_bias <span class="token operator">=</span> relative_position_bias<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># nH, Wh*Ww, Wh*Ww</span>
    attn <span class="token operator">=</span> attn <span class="token operator">+</span> relative_position_bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        nW <span class="token operator">=</span> mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B_ <span class="token operator">//</span> nW<span class="token punctuation">,</span> nW<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> N<span class="token punctuation">,</span> N<span class="token punctuation">)</span> <span class="token operator">+</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> N<span class="token punctuation">,</span> N<span class="token punctuation">)</span>
        attn <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        attn <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>

    attn <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_drop<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>

    x <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B_<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword">return</span> x</span></span></span></code></pre><h3 id="https://www.notion.so/67fc91d8f3a74a4ea635deef554637a6" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/67fc91d8f3a74a4ea635deef554637a6"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">window_reverse</span></span></h3><div id="https://www.notion.so/ed991c684f95454c8a7943debb2f1703" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">将一个个window还原成一个feature map</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/58ba9ed0b297432eaa752b2f22815d5e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">首先，根据H_sp和W_sp计算出每张图像划分出的窗口数量，即H * W / H_sp / W_sp，并用它除以B’得到批量大小B。</span></span></li><li id="https://www.notion.so/f5acf1ceee3349a69f531dfb7a1a54db" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">然后，将输入张量img_splits_hw重塑为(B, H // H_sp, W // W_sp, H_sp, W_sp, C)的形状，其中每个维度分别表示批量大小，窗口行数，窗口列数，窗口高度，窗口宽度和通道数。</span></span></li><li id="https://www.notion.so/5c019786a26b4939bdec6d889d16a5a3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">然后，将img在第二维和第四维上进行交换，得到(B, W // W_sp, H // H_sp, H_sp, W_sp, C)的形状，并将其展平为(B, C, H, W)的形状。这样就实现了将每个窗口内部的元素按照原始图像的顺序排列，并合并成一个完整的图像。</span></span></li></ul><div id="https://www.notion.so/c6d873b3d96d4185b0462ddfba62dd99" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><pre id="https://www.notion.so/bfcdc4ff5f954c94ac482a062af79115" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">def</span> <span class="token function">windows2img</span><span class="token punctuation">(</span>img_splits_hw<span class="token punctuation">,</span> H_sp<span class="token punctuation">,</span> W_sp<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    img_splits_hw: B' H W C
    """</span>
    B <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>img_splits_hw<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token punctuation">(</span>H <span class="token operator">*</span> W <span class="token operator">/</span> H_sp <span class="token operator">/</span> W_sp<span class="token punctuation">)</span><span class="token punctuation">)</span>

    img <span class="token operator">=</span> img_splits_hw<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H <span class="token operator">//</span> H_sp<span class="token punctuation">,</span> W <span class="token operator">//</span> W_sp<span class="token punctuation">,</span> H_sp<span class="token punctuation">,</span> W_sp<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    img <span class="token operator">=</span> img<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>H<span class="token punctuation">,</span>W<span class="token punctuation">)</span>
    <span class="token keyword">return</span> img</span></span></span></code></pre><blockquote id="https://www.notion.so/30fed9c7ad9443c999175be3643c45b3" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://zhuanlan.zhihu.com/p/367111046">图解Swin Transformer - 知乎 (zhihu.com)</a></span></span></blockquote><h2 id="https://www.notion.so/7b53ca2c4587430682870178357d5866" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/7b53ca2c4587430682870178357d5866"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Swin transformer V2</span></span></h2><blockquote id="https://www.notion.so/cbc20a9198d549b28b634a64b582d7c9" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://sh-tsang.medium.com/review-swin-transformer-v2-scaling-up-capacity-and-resolution-401c28b02df8">https://sh-tsang.medium.com/review-swin-transformer-v2-scaling-up-capacity-and-resolution-401c28b02df8</a></span></span></blockquote><div id="https://www.notion.so/93784b43cd844e37be4ba39b2febfcfc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">本文是Swin-T团队在Swin-T模型的基础上对scale up视觉模型的一个工作，在4个数据集上又重新刷到了新的SOTA文章的出发点是，在视觉领域里并没有像NLP那样，对于增大模型scale有比较好的探索，文中讲到可能的原因是：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/81295b7dd88a499d95a08a71d2f83df6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">在</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">增大视觉模型的时可能会带来很大的训练不稳定性</strong></span></span></li><li id="https://www.notion.so/7ccc3282d5034347aca9779b09f35212" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">在很多需要高分辨率的下游任务上，还没有很好的探索出来</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">对低分辨率下训练好的模型迁移到更大scale模型上</strong></span><span class="SemanticString">的方法</span></span></li></ul><h3 id="https://www.notion.so/9764cc37994644ed8690fe056ab6fe46" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/9764cc37994644ed8690fe056ab6fe46"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">三种技术</span></span></h3><div id="https://www.notion.so/2faabb203f984750ab0d98c90446b4a6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">提出了三种主要技术：1) 结合余弦注意力的残差后规范方法，以提高训练的稳定性；2) 对数间隔连续位置偏置方法，以有效地将使用低分辨率图像预训练的模型转移到高分辨率输入的下游任务中；3) 自监督预训练方法 SimMIM，以减少对大量标记图像的需求。</span></span></p></div><div id="https://www.notion.so/5127ab8583e4465cb333567a3149da64" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa8cdbe71-2f5b-4ea6-a395-265c789c4a19%2FUntitled.png?width=624&amp;table=block&amp;id=5127ab85-83e4-465c-b333-567a3149da64"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa8cdbe71-2f5b-4ea6-a395-265c789c4a19%2FUntitled.png?width=624&amp;table=block&amp;id=5127ab85-83e4-465c-b333-567a3149da64" style="width:624px"/></a><figcaption><span class="SemanticStringArray"><span class="SemanticString">为了更好地扩展模型容量和窗口分辨率，我们对原有的 Swin Transformer 架构（V1）进行了多项调整：1) 以重后规范取代之前的前规范配置；2) 以比例余弦关注取代原来的点积关注；3) 以对数间隔的连续相对位置偏置方法取代之前的参数化方法。适应性 1) 和 2) 使模型更容易扩大容量。适应性 3) 使模型能更有效地跨窗口分辨率转移。经过调整的架构被命名为 Swin Transformer V2。</span></span></figcaption></figure></div><div id="https://www.notion.so/6dedbcfe364543d7bce90e2ed0e0a72d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/b33869299bbc4ca9a52a43084687ab76" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/b33869299bbc4ca9a52a43084687ab76"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Post normalization</span></span></h3><div id="https://www.notion.so/31af5c0cc5714b459645811e3a9417c5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">为缓解这一问题，我们建议采用残差后归一化方法，如图 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Resource"><a href=""><span class="SemanticStringArray"><span class="SemanticString">Unknown Page</span></span></a></span></span><span class="SemanticString"> 所示。在这种方法中，每个残差块的输出在合并回主分支之前都会进行归一化处理，当层深入时，主分支的振幅不会累积。如图 </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.notion.so/402fabb8d72a44e8a65f5977ac393f14?pvs=25#fae13bce13f247ddb64e749ff4d7d8bb">2</a></span><span class="SemanticString"> 所示，这种方法的激活振幅比原来的预归一化配置要温和得多。</span></span></p></div><div id="https://www.notion.so/fbdb46061e764fbcb7ad6bb308e1c440" class="ColumnList"><div id="https://www.notion.so/252f41fecadf4289a387da90bea6ab7c" class="Column" style="width:calc((100% - var(--column-spacing) * 1) * 0.5)"><div id="https://www.notion.so/888edea4ac604f1fb423843d6840b887" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fbfd00ace-0712-43fd-9f71-95c35b16f066%2FUntitled.png?width=192&amp;table=block&amp;id=888edea4-ac60-4f1f-b423-843d6840b887"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fbfd00ace-0712-43fd-9f71-95c35b16f066%2FUntitled.png?width=192&amp;table=block&amp;id=888edea4-ac60-4f1f-b423-843d6840b887" style="width:192px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div></div><div id="https://www.notion.so/92b72f38305645eca558b99875c33159" class="Column" style="width:calc((100% - var(--column-spacing) * 1) * 0.5)"><div id="https://www.notion.so/04b7dd5d19a04f10b412f74057deb04a" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F3fffe886-4acf-4995-aec8-f98ae1d10ec3%2FUntitled.png?width=188&amp;table=block&amp;id=04b7dd5d-19a0-4f10-b412-f74057deb04a"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F3fffe886-4acf-4995-aec8-f98ae1d10ec3%2FUntitled.png?width=188&amp;table=block&amp;id=04b7dd5d-19a0-4f10-b412-f74057deb04a" style="width:188px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div></div></div><div id="https://www.notion.so/fae3bb7e88a0486385180d36cb386cbc" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fce393c7c-32e2-4f7c-8775-ea63da4e03f7%2FUntitled.png?width=565&amp;table=block&amp;id=fae3bb7e-88a0-4863-8518-0d36cb386cbc"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fce393c7c-32e2-4f7c-8775-ea63da4e03f7%2FUntitled.png?width=565&amp;table=block&amp;id=fae3bb7e-88a0-4863-8518-0d36cb386cbc" style="width:565px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h3 id="https://www.notion.so/e141c8c14bf54eb89cfc959ac3be0f1e" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/e141c8c14bf54eb89cfc959ac3be0f1e"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Scaled cosine attention</span></span></h3><div id="https://www.notion.so/7b11c893e13745d6b09d1637f532f91b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在最初的自我注意力计算中，像素对的相似性项是作为查询向量和关键向量的点积来计算的。我们发现，在大型视觉模型中使用这种方法时，一些区块和头部的学习注意力图经常被少数像素对所支配，尤其是在重后规范配置中。为了缓解这一问题，我们提出了一种缩放余弦注意力方法，即通过缩放余弦函数计算像素对 i 和 j 的注意力对数：</span></span></p></div><div id="https://www.notion.so/6802513d3afa4592b02deee1f002eddb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="Sim(q_i,k_j) = cos(q_i,k_j)/ \tau+B_{ij}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>i</mi><mi>m</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>k</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>k</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>τ</mi><mo>+</mo><msub><mi>B</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Sim(q_i,k_j) = cos(q_i,k_j)/ \tau+B_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.1132em;">τ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05017em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></p></div><div id="https://www.notion.so/0eaae1a0648c44c1b68b031d58055568" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">其中，Bij 是像素 i 和 j 之间的相对位置偏差；τ 是一个可学习的标量，不跨头和层共享。</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">余弦函数是自然归一化的，因此可以有较温和的注意力值。</strong></span></span></p></div><div id="https://www.notion.so/bd465809e4804796a54647a2d3ddfcfe" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/6c8c529f26f4455ca98d9ed29b0649ba" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/6c8c529f26f4455ca98d9ed29b0649ba"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">原始参数化相对位置偏差</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/01b17e23c1c14325accc872c242c82ae" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">沿每个轴的相对位置在[- </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">M</strong></em></span><span class="SemanticString"> </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">+1,</strong></span><span class="SemanticString"> </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">M</strong></em></span><span class="SemanticString"> </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">+1]范围</strong></span><span class="SemanticString">内，并且相对位置偏差被参数化为</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">偏差矩阵 ^ </strong></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">B</strong></em></span><span class="SemanticString">。</span></span></li><li id="https://www.notion.so/2ac7942d12334eefb3891187fa49d968" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">当跨不同窗口大小进行迁移时</strong></span><span class="SemanticString">，预训练中</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">学习到的相对位置偏置矩阵</strong></span><span class="SemanticString">用于</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">通过双三次插值来初始化微调</strong></span><span class="SemanticString">中</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">不同大小的偏置矩阵。</strong></span></span></li></ul><h3 id="https://www.notion.so/78aed22194e54896b287e8c8b9fb5f5d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/78aed22194e54896b287e8c8b9fb5f5d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Swin V2 线性间隔连续位置偏置 (CPB)</span></span></h3><div id="https://www.notion.so/d36914748e314d1bbf28383f7bb8a346" class="Image Image--Normal"><figure><a href="#?width=144"><img src="#?width=144" style="width:144px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/f4d5ebf2729a4fa5a8aaf0150052b513" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Swin V2 线性间隔 CPB。</strong></span><span class="SemanticString">（从 Swin V2 块裁剪）</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/19fd9e2d5d4540299a50e9675265c2a8" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">连续</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">位置偏差（CPB）方法在相对坐标上采用小型元网络：</strong></span></span></li></ul><div id="https://www.notion.so/f7051d56ed084dffbfdc286060f2f53b" class="Image Image--Normal"><figure><a href="#?width=288"><img src="#?width=288" style="width:288px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/b1a566f2bd434671baac757c62e6b022" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">其中</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">G</em></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">是一个</strong></span><span class="SemanticString">小型网络，例如默认情况下具有</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://sh-tsang.medium.com/brief-review-rectified-linear-units-improve-restricted-boltzmann-machines-43724ede3ecf">ReLU激活的</a></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">2 层 MLP 。</strong></span><span class="SemanticString">（它是Swin V2块图中的“MLP”。）</span></span></li><li id="https://www.notion.so/1a38f215a3384c0289edb49e230866ae" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">元</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">网络_G_为任意相对坐标 生成偏差值</strong></span><span class="SemanticString">，因此可以</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">自然地转移到具有任意变化窗口大小的微调任务。</strong></span></span></li><li id="https://www.notion.so/cdf28f77bf674d0db3487b32c2826d94" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">在推论中，可以预先计算</strong></span><span class="SemanticString">每个相对位置的偏差值并将其存储为模型参数。</span></span></li></ul></article>
  <footer class="Footer">
  <div>&copy; Simon’s Blogs 2022</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>