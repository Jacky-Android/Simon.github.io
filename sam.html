<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="https://avatars.githubusercontent.com/u/55181594?v=4">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>Segment Anything&nbsp;|&nbsp;Simon’s Blogs</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="Segment Anything">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📧&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="https://avatars.githubusercontent.com/u/55181594?v=4"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😀&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About me</span>
        </div>
      </a>
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📧&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">Segment Anything</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Tue, Dec 5, 2023</span>
        
        
      </div>
    
  </header>
  <article id="https://www.notion.so/66c2bd8981d34aadbdcf650be8bc508e" class="PageRoot"><h1 id="https://www.notion.so/3d257f46bcfe47f0aad318a5cf1b7ce1" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/3d257f46bcfe47f0aad318a5cf1b7ce1"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">背景</span></span></h1><div id="https://www.notion.so/891e07090ed34164900100bc1a67fddf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">近年来，基础模型取得了显著的成功，特别是通过</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">大型语言模型</strong></span><span class="SemanticString">（</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">LLMs</code></span><span class="SemanticString">），主要归因于</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">数据</strong></span><span class="SemanticString">和</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">模型</strong></span><span class="SemanticString">规模的大幅扩展。例如，像</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">GPT-4</code></span><span class="SemanticString">这样的十亿参数模型已成功用于零/少样本学习，而无需大量的任务特定数据或模型参数更新。与此同时，有5400亿参数的</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">Pathways Language Model（PaLM）</code></span><span class="SemanticString">在许多领域展现了先进的能力，包括语言理解、生成、推理和与代码相关的任务。</span></span></p></div><div id="https://www.notion.so/2608add0d520438f8fe9ac30d3ee1fca" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">反观视觉领域，诸如</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">CLIP</code></span><span class="SemanticString">这样的预训练视觉语言模型在不同的下游视觉任务上展现了强大的零样本泛化性能。这些模型通常使用从网络收集的数百上千万</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">图像-文本对</strong></span><span class="SemanticString">进行训练，并提供具有泛化和迁移能力的表示。因此，只需通过简单的自然语言描述和提示，这些预训练的基础模型完全被应用到下游任务，例如使用精心设计的提示进行零样本分类。</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">（but CLIP没有公开）</strong></mark></span></span></p></div><div id="https://www.notion.so/248edd69da1e4c7a9ae84668397205a7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">除了此类大型视觉语言基础模型外，一些研究工作也致力于开发可以通过</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">视觉输入提示</strong></span><span class="SemanticString">的大型基础模型。例如，META 推出的 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">SAM</code></span><span class="SemanticString"> 能够执行与类别无关的分割，给定图像和视觉提示（如框、点或蒙版），指定要在图像中分割的内容。这样的模型可以轻松适应特定的下游任务，如</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">医学图像分割</strong></span><span class="SemanticString">、</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">视频对象分割</strong></span><span class="SemanticString">、</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">机器人技术</strong></span><span class="SemanticString">和</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">遥感</strong></mark></span><span class="SemanticString">等。</span></span></p></div><div id="https://www.notion.so/be38259dff1d4b1eb25c01d469dc23cf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/3c8488d221f24b69933e3fc3823a636b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Segment Anything (SAM) 项目：一个用于图像分割的新任务、模型和数据集。通过在数据收集循环中使用的高效模型，SAM建立了迄今为止最大的分割数据集（迄今为止），其中包含 1100 万张授权图像上的 10 亿多个掩码，并且尊重隐私。该模型的设计和训练具有可提示性，因此它可以在新的图像分布和任务中进行零转移。SAM在大量任务中评估了它的能力，发现它的Zero-shot性能令人印象深刻--通常可与之前的完全监督结果相媲美，甚至更胜一筹。</span></span></p></div><div id="https://www.notion.so/974361efb34b47519485956b43752794" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/c4e05203d3734ff5b51bd892042395a9" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/c4e05203d3734ff5b51bd892042395a9"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SegGPT</strong></strong></span></span></h1><div id="https://www.notion.so/6389db32c4984f4784099eed49ba301f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/c77b854f399c4374b1bc2b5d49e54748" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa5b3301f-c6c5-42bf-b85d-95145acb9065%2FUntitled.png?width=1204&amp;table=block&amp;id=c77b854f-399c-4374-b1bc-2b5d49e54748"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa5b3301f-c6c5-42bf-b85d-95145acb9065%2FUntitled.png?width=1204&amp;table=block&amp;id=c77b854f-399c-4374-b1bc-2b5d49e54748" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/acd001377c6349b5b66128bd4fbf632d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">SegGPT</code></span><span class="SemanticString"> 旨在训练一个通用模型，可以用于解决所有的分割任务，其训练被制定为一个上下文着色问题，为每个数据样本随机分配颜色映射。目标是根据上下文完成不同的分割任务，而不是依赖于特定的颜色。</span></span></p></div><div id="https://www.notion.so/55e87e2cf05142da9fb12c4ddd8b4f0a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/254129f188404204894ff862123f4577" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/254129f188404204894ff862123f4577"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SAM</strong></strong></span></span></h1><div id="https://www.notion.so/08b1e025e92d430a8f4deaf8dd6dfb44" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/0e9aa5318e684aea9af386166a1e8a23" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">概述</strong></span><span class="SemanticString">：SAM 是一种零样本分割模型，从头开始训练，不依赖于 CLIP。 </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">结构</strong></span><span class="SemanticString">：</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray">使用图像和提示编码器对图像和视觉提示进行编码，然后在轻量级掩码解码器中组合以预测分割掩码</mark></strong></span><span class="SemanticString">。 </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">训练方法</strong></span><span class="SemanticString">：通过三阶段的数据注释过程（辅助手动、半自动和全自动）训练。</span></span></p></div><div id="https://www.notion.so/b92b0cde9fae4f1fbc42ebc6752a5ae8" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fb78db24a-7eac-4c27-914a-250445e9b338%2FUntitled.png?width=1237&amp;table=block&amp;id=b92b0cde-9fae-4f1f-bc42-ebc6752a5ae8"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fb78db24a-7eac-4c27-914a-250445e9b338%2FUntitled.png?width=1237&amp;table=block&amp;id=b92b0cde-9fae-4f1f-bc42-ebc6752a5ae8" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/507dec8ca3b943db9d326217d226782a" class="Image Image--PageWidth"><figure><a href="https://github.com/lucidrains/vit-pytorch/raw/main/images/vit.gif"><img src="https://github.com/lucidrains/vit-pytorch/raw/main/images/vit.gif" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/4e31d54aeab1470b94ced3663571cfde" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/be5bbdf543114253b09160c28522686d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/be5bbdf543114253b09160c28522686d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SEEM</strong></strong></span></span></h1><div id="https://www.notion.so/1fe6efbbd88647e0a1efe0261eba79de" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F74d853a9-486b-4850-a346-b1d2adbcdb73%2FUntitled.png?width=1319&amp;table=block&amp;id=1fe6efbb-d886-47e0-a1ef-e0261eba79de"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F74d853a9-486b-4850-a346-b1d2adbcdb73%2FUntitled.png?width=1319&amp;table=block&amp;id=1fe6efbb-d886-47e0-a1ef-e0261eba79de" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/ae416d6a86434978ac338b0847a8bbe9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/782ca4bc621a4fc180393135c1cd9879" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">与 SAM 相比，SEEM 涵盖了更广泛的交互和语义层面。例如，SAM 只支持有限的交互类型，如点和框，而由于它本身不输出语义标签，因此错过了高语义任务。</span></span></p></div><div id="https://www.notion.so/90aac951ba3d462aba37514b7c0a0352" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">首先，</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">SEEM</code></span><span class="SemanticString"> </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedColor SemanticString__Fragment--ColorRed">有一个统一的提示编码器，将所有视觉和语言提示编码到联合表示空间中。因此，SEEM 可以支持更通用的用途。它有潜力扩展到自定义提示。其次，SEEM 在文本掩码（基础分割）方面非常有效，并输出语义感知预测。</mark></strong></span></span></p></div><div id="https://www.notion.so/5fe68b4c20ce454eb9f048866c04c084" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/425ee03d0df84136979a34b65811ef97" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/425ee03d0df84136979a34b65811ef97"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SAM 的改进与应用</strong></span></span></h1><h2 id="https://www.notion.so/f66ae8957c62444f9aef3898524e30c7" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/f66ae8957c62444f9aef3898524e30c7"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SAM for Medical Segmentation</strong></span></span></h2><h3 id="https://www.notion.so/80694fa6fba042f292d3ff1ecce48f27" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/80694fa6fba042f292d3ff1ecce48f27"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Adapting by Fine-Tuning</strong></span></span></h3><blockquote id="https://www.notion.so/a60d4997346b4b67a58f6cfea9fffc66" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/2304.12306.pdf">https://arxiv.org/pdf/2304.12306.pdf</a></span></span><div id="https://www.notion.so/5d8351d24c5f47b6a898721767368c19" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Title: Segment Anything in Medical Images</strong></mark></span></span></p></div><div id="https://www.notion.so/e4ec84291f274001874e1a12b5bc58d5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">code：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/bowang-lab/MedSAM">https://github.com/bowang-lab/MedSAM</a></span></span></p></div></blockquote><div id="https://www.notion.so/db919a62503b425b9fd5650820019338" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">MedSAM</code></span><span class="SemanticString">：通过在大规模医学分割数据集上微调 SAM，创建了一个用于通用医学图像分割的扩展方法 MedSAM。这一方法在 21 个 3D 分割任务和 9 个 2D 分割任务上优于 SAM。</span></span></p></div><div id="https://www.notion.so/b9883a2e4dd1430a913577a5925e7803" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Ff3a3349c-b87f-4d81-a8d3-5d5272807bcf%2FUntitled.png?width=805&amp;table=block&amp;id=b9883a2e-4dd1-430a-9135-77a5925e7803"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Ff3a3349c-b87f-4d81-a8d3-5d5272807bcf%2FUntitled.png?width=805&amp;table=block&amp;id=b9883a2e-4dd1-430a-9135-77a5925e7803" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/49ce5e044a5247e2bb42c4dd4f483b3a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/65517797dc4c4c06b179ba0de13e5c6d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/3116df2d938d405eb27c793ba65e1d46" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/3116df2d938d405eb27c793ba65e1d46"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Adapting through Auxiliary Prompt Encoder</strong></span></span></h3><blockquote id="https://www.notion.so/c06bc2306b2641349582d8450a7ea737" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/2306.13731.pdf">https://arxiv.org/pdf/2306.13731.pdf</a></span></span><div id="https://www.notion.so/43dcc9aa9c46492a81c363379e83a599" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Title：</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray">How to Efficiently Adapt Large Segmentation Model(SAM) to Medical Images</mark></strong></span></span></p></div><div id="https://www.notion.so/6df231a176004c028c3cbba7d3eaa158" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">code：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/xhu248/AutoSAM">https://github.com/xhu248/AutoSAM</a></span></span></p></div></blockquote><div id="https://www.notion.so/0d71384ee2e643599858334eec61d137" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">AutoSAM</code></span><span class="SemanticString">：为</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">SAM</code></span><span class="SemanticString">的提示生成了一个完全自动化的解决方案，基于输入图像由</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">AutoSAM</code></span><span class="SemanticString">辅助提示编码器网络生成替代提示。AutoSAM 与原始的 SAM 相比具有更少的可训练参数，同时冻结了image encoder的参数。</span></span></p></div><div id="https://www.notion.so/b81509abc1504cfda3fd9d178174db88" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F9c79c3c3-c5ef-4362-90eb-0aaf747ea59d%2FUntitled.png?width=1036&amp;table=block&amp;id=b81509ab-c150-4cfd-a3fd-9d178174db88"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F9c79c3c3-c5ef-4362-90eb-0aaf747ea59d%2FUntitled.png?width=1036&amp;table=block&amp;id=b81509ab-c150-4cfd-a3fd-9d178174db88" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/b7aa2c7ecaab46cf91b2aef9b2979442" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><blockquote id="https://www.notion.so/0c13280eaf7c4d078416a53d052e5a53" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">Title：</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray">Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation</mark></strong></span></span><div id="https://www.notion.so/75916d202fd24bbbbc1379219a4c7417" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/2308.14936.pdf">https://arxiv.org/pdf/2308.14936.pdf</a></span></span></p></div><div id="https://www.notion.so/bf2f66180e804f1591a8f1d14fd37e17" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">code：无</span></span></p></div></blockquote><div id="https://www.notion.so/63bfef7e1c244d2b8a613c3e61033f82" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">AutoSAM Adapter：</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedColor SemanticString__Fragment--ColorRed">第一个使SAM适用于基于 CT 的 3D 多器官分割网络，采用参数高效的适应技术来开发自动提示学习范例</mark></span><span class="SemanticString">，以促进 SAM 模型的功能向 3D 医学图像分割的转换，从而无需手动生成提示，性能表现SOTA！采用参数高效的适应技术来开发自动提示学习范例，以促进 SAM 模型的功能向 3D 医学图像分割的转换，从而无需手动生成提示。 此外，我们有效地将获得的 AutoSAM 适配器知识迁移到专为 3D 医学图像分析量身定制的其他轻量级模型，在医学图像分割任务上实现最先进的 (SOTA) 性能。</span></span></p></div><div id="https://www.notion.so/f55af1f1a09948329d51943d55d89682" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa2e86c1e-2ba8-4144-a6dd-840fd204a415%2FUntitled.png?width=1376&amp;table=block&amp;id=f55af1f1-a099-4832-9d51-943d55d89682"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa2e86c1e-2ba8-4144-a6dd-840fd204a415%2FUntitled.png?width=1376&amp;table=block&amp;id=f55af1f1-a099-4832-9d51-943d55d89682" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/08a7c29301d04236978ae748a40489ed" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/51436adffbdd42c89963aecd7c3e0600" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/51436adffbdd42c89963aecd7c3e0600"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Adapting Through Adapters</strong></span></span></h3><blockquote id="https://www.notion.so/a0c5d6cf5df1420a9633a255a5332228" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/2304.13425">https://arxiv.org/abs/2304.13425</a></span></span><div id="https://www.notion.so/3e9393ae962a42d39245bc70d86438dd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Title：</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Learnable Ophthalmology SAM</strong></mark></span></span></p></div><div id="https://www.notion.so/e20e9950791c4fba85c06639289f80fb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">code：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/Qsingle/LearnablePromptSAM">https://github.com/Qsingle/LearnablePromptSAM</a></span></span></p></div></blockquote><div id="https://www.notion.so/520add02a18b467c863afec312e08af0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Learnable Ophthalmology SAM：基于 Segment Anything（SAM），提出了一种简单而有效的可学习提示层，适用于眼科多模态图像中的多目标分割，命名为Learnable Ophthalmology Segment Anything Segment Anything（SAM）。</span></span></p></div><div id="https://www.notion.so/33e399e355b749b1902bbb23c5dc3019" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Faa76e08f-5136-4986-b73a-85aa887f9b07%2FUntitled.png?width=413&amp;table=block&amp;id=33e399e3-55b7-49b1-902b-bb23c5dc3019"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Faa76e08f-5136-4986-b73a-85aa887f9b07%2FUntitled.png?width=413&amp;table=block&amp;id=33e399e3-55b7-49b1-902b-bb23c5dc3019" style="width:413px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/92943483133e4f77869d6bcc415a7c22" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><blockquote id="https://www.notion.so/6cbdd576a81e45f8b19494c2b868a052" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/2304.12620.pdf">https://arxiv.org/pdf/2304.12620.pdf</a></span></span><div id="https://www.notion.so/40c2e80bb9724102942aa7c2caeeb569" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Title：</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation</strong></mark></span></span></p></div><div id="https://www.notion.so/215e2a0f87194531b35b0856f01e01b6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">code：</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">‣</span></span></span></p></div></blockquote><div id="https://www.notion.so/8b403056c49b47cfae497f594b790653" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">Medical SAM Adapter</code></span><span class="SemanticString">：专为SAM设计了一个通用的医学图像分割适配器，能够适应医学数据的高维度（3D）以及独特的视觉提示，如 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">point</code></span><span class="SemanticString"> 和 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">box</code></span><span class="SemanticString">。</span></span></p></div><div id="https://www.notion.so/31f44a1f3a324625aea9dbea4627acd3" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fac91e72e-843b-47c5-bf5a-e625b66177d6%2FUntitled.png?width=911&amp;table=block&amp;id=31f44a1f-3a32-4625-aea9-dbea4627acd3"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fac91e72e-843b-47c5-bf5a-e625b66177d6%2FUntitled.png?width=911&amp;table=block&amp;id=31f44a1f-3a32-4625-aea9-dbea4627acd3" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/66b1e99855e845fb874ad05b69a16b46" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/4ff68c13d1e54d649089a6b3a3b1b859" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/4ff68c13d1e54d649089a6b3a3b1b859"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Adapting by Modifying SAM’s Decoder</strong></span></span></h3><blockquote id="https://www.notion.so/f5f9b6a6bb7b4b829184c155e1c26ed8" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/2306.00499.pdf">https://arxiv.org/pdf/2306.00499.pdf</a></span></span><div id="https://www.notion.so/0cf4495f66d24d738402842cd9d945c5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Title：</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">DeSAM: Decoupling Segment Anything Model for Generalizable Medical Image Segmentation</strong></mark></span></span></p></div><div id="https://www.notion.so/39ebba64c4a44475b5da11a6c647cd8d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">code：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/yifangao112/DeSAM">https://github.com/yifangao112/DeSAM</a></span></span></p></div></blockquote><div id="https://www.notion.so/78705f700bc94098b6fa6851fb4c6015" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F02faba40-5e20-4769-baab-7b2a2ae9ac3b%2FUntitled.png?width=1438&amp;table=block&amp;id=78705f70-0bc9-4098-b6fa-6851fb4c6015"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F02faba40-5e20-4769-baab-7b2a2ae9ac3b%2FUntitled.png?width=1438&amp;table=block&amp;id=78705f70-0bc9-4098-b6fa-6851fb4c6015" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/98b8b243d10d4497a42f53047a1139b1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">基于</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">分割任意模型</strong></span><span class="SemanticString">（SAM），一种具有强大泛化能力的视觉基础模型，对其掩膜解码器进行修改，使其能够解耦掩膜生成和提示嵌入，从而减少不良提示的影响。同时，利用预训练的权重，避免对重量级的图像编码器进行调整。</span></span></p></div><div id="https://www.notion.so/c7cdd51eb2c949a7a3c0566e7fa0272a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><blockquote id="https://www.notion.so/1a2932727d7f4a6ba0a2334355d220eb" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">Paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/2311.08190">https://arxiv.org/abs/2311.08190</a></span></span><div id="https://www.notion.so/35883a178e964727a9fc3ad756e9e71d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Title：</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage Segmentation</strong></mark></span></span></p></div><div id="https://www.notion.so/728842658e5d488288724e755d2de7ef" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Code：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/mileswyn/SAMIHS">https://github.com/mileswyn/SAMIHS</a></span></span></p></div></blockquote><div id="https://www.notion.so/1fad614f746f4236998380a81dd4e84d" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F160b909f-4e1d-403a-b810-d6b18299a0e0%2FUntitled.png?width=1470&amp;table=block&amp;id=1fad614f-746f-4236-9983-80a81dd4e84d"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F160b909f-4e1d-403a-b810-d6b18299a0e0%2FUntitled.png?width=1470&amp;table=block&amp;id=1fad614f-746f-4236-9983-80a81dd4e84d" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/0e673676c85341f9a74471eb7e6d2459" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">与先前的SAM和基于SAM的方法不同，SAMIHS将参数重构适配器引入SAM的图像编码器，并考虑适配器参数的高效灵活利用。此外，我们使用了一个组合损失，将二元交叉熵损失和边界敏感损失结合起来，以增强SAMIHS对边界区域的识别能力。</span></span></p></div><h1 id="https://www.notion.so/ccae9575647248bc961a3ea8bad149da" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/ccae9575647248bc961a3ea8bad149da"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SAM for Remote Sensing</strong></strong></span></span></h1><div id="https://www.notion.so/b5076cbf0c334bbc9357fbcceae8ea2d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">SAM 在遥感图像分割方面的应用集中在通过点、框和粗粒度掩码的引导来理解和分割遥感图像。以下是 SAM 在遥感分割方面的应用以及相关挑战。</span></span></p></div><h2 id="https://www.notion.so/8b93947fd1d743a8b7f9c29d6ed76546" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/8b93947fd1d743a8b7f9c29d6ed76546"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SAM在遥感分割的基本应用</strong></span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/62ec3d315b0e450c899595e5c44b76be" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">交互性质：由于 SAM 的交互特性，它主要依赖于点、框和粗粒度掩码的手动引导。</span></span></li><li id="https://www.notion.so/d02313e03d3e4eaaaa92ad437c2f1833" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">限制：</span></span><ol class="NumberedListWrapper"><li id="https://www.notion.so/9ff6a725d7f7436ab9fee14e4f9ddba7" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">全自动分割困难：SAM在完全自动地理解遥感图像方面效果不佳。</span></span></li><li id="https://www.notion.so/2cee066ec7814c9ea0c50e986cfb8be1" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">结果依赖性：SAM的结果严重依赖于用于分割遥感图像目标的提示的类型、位置和数量。</span></span></li><li id="https://www.notion.so/1e6efc5410354bf3adb89f6b840e83fb" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">手动提示优化需求：要实现理想的结果，通常需要对手动提示进行精炼。</span></span></li><li id="https://www.notion.so/69d45642016048d5b77069a517e1cf04" class="NumberedList" value="4"><span class="SemanticStringArray"><span class="SemanticString">遥感分割需要准确知道其类别，应用于场景的土地规划和统计。</span></span></li></ol></li></ul><div id="https://www.notion.so/21252910e64a4cdba84def096250d03d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><blockquote id="https://www.notion.so/73e732fd9eca4e10966425d2a1f9810e" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/2306.16269.pdf">https://arxiv.org/pdf/2306.16269.pdf</a></span></span><div id="https://www.notion.so/58f6d900afb54e27a8d55c6299be1ae6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Title：</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgGray"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</strong></mark></span></span></p></div><div id="https://www.notion.so/2cb245d58f8b4b059405d630749ab2e3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">code：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/KyanChen/RSPrompter">https://github.com/KyanChen/RSPrompter</a></span></span></p></div></blockquote><div id="https://www.notion.so/623c23f228e943df84ddb1c2d8c44e31" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">概述</strong></span><span class="SemanticString">：RsPrompter 是一个将语义分类信息与 SAM 结合的方法，用于遥感图像的自动实例分割。 </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">操作方式</strong></span><span class="SemanticString">：</span></span></p></div><div id="https://www.notion.so/fd316b061a924322af9b9884582d9768" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">学习生成提示：RsPrompter 提出了一种学习生成适当的SAM输入提示的方法。 生成提示包含的信息：通过分析编码器的中间层来生成包含关于语义类别的信息的提示，并生成提示嵌入，这可以视为点或框嵌入。 目标：通过自动化生成适当的输入提示，RsPrompter 试图克服 SAM 在遥感图像分割方面的局限性。</span></span></p></div><div id="https://www.notion.so/04247c4e99c141f698bf6d992d9955c0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">尽管 SAM 在遥感图像分割方面存在一些限制，主要与其交互性质和对手动引导的依赖有关，但通过引入如 RsPrompter 这样的方法，可以利用 SAM 实现遥感图像的自动实例分割。这些努力标志着朝着减少人工干预和提高遥感图像分析自动化的方向迈出的重要一步，有势必推动遥感科学、地理信息系统（GIS）和环境监测等领域的进展。</span></span></p></div><div id="https://www.notion.so/cbc9e04a95144158a2610a4f2b8e2642" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fcd756fa0-8aa7-452b-877a-c7c7faaf76a8%2FUntitled.png?width=1263&amp;table=block&amp;id=cbc9e04a-9514-4158-a261-0a4f2b8e2642"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fcd756fa0-8aa7-452b-877a-c7c7faaf76a8%2FUntitled.png?width=1263&amp;table=block&amp;id=cbc9e04a-9514-4158-a261-0a4f2b8e2642" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/5030091138af416fb98024883535c42d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><blockquote id="https://www.notion.so/802d691fc0734376ada6c8303838abd8" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">Paper:</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/2311.15138v1.pdf">Can SAM recognize crops? Quantifying the zero-shot performance of a semantic segmentation foundation model on generating crop-type maps using satellite imagery for precision agriculture</a></span></span></blockquote><div id="https://www.notion.so/3cd9908afa3b495386e914d7b730968b" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa3514889-9645-442b-9a74-f4003b7f448b%2FUntitled.png?width=1201&amp;table=block&amp;id=3cd9908a-fa3b-4953-86e9-14d7b730968b"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fa3514889-9645-442b-9a74-f4003b7f448b%2FUntitled.png?width=1201&amp;table=block&amp;id=3cd9908a-fa3b-4953-86e9-14d7b730968b" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/9eb13e6e7fc24e7f92ec1012e5649f1a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这篇论文解决了农业产业中的一个问题和挑战，即如何利用遥感卫星图像快速、准确地生成作物类型的地图。传统上，生成作物类型地图是一项具有挑战性和成本高昂的任务。这篇论文介绍了使用Meta AI的Segment Anything Model（SAM）来进行作物地图预测的研究，并考虑了SAM在零样本图像分割方面的成功应用。然而，由于SAM在输入通道数量上的限制以及其零样本使用的类别不可知性，直接使用SAM进行作物类型映射面临一些挑战。为了评估SAM在分割卫星图像和生成作物类型地图方面的零样本性能，论文中提出了使用聚类一致性指标的方法。实验证明，虽然在零样本设置下直接利用SAM进行作物类型映射具有一定困难，但SAM在迅速且准确地勾勒卫星图像中的农田方面具有潜力，并可作为后续作物分类的基础。这篇论文试图展示了像SAM这样的最先进图像分割模型在作物类型映射和农业产业相关需求方面的使用案例，为精准农业实践提供了潜在的自动化、高效和经济实惠的数据产品途径。</span></span></p></div><div id="https://www.notion.so/87c78d08fea94d0a94150a42d9ea4ff7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><blockquote id="https://www.notion.so/2cfff356e02e40faa49af5dc07248f27" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">Paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://arxiv.org/abs/2311.11319">http://arxiv.org/abs/2311.11319</a></span></span><div id="https://www.notion.so/e150b5be0e2e4487bd90fe6eeff94944" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Title: GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for
Automated Segmentation of Mobility Infrastructure</span></span></p></div></blockquote><div id="https://www.notion.so/adac8c801e864775b4769913e591b7ad" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fe7f3cc17-40e1-48bc-9b03-23dd725190e1%2FUntitled.png?width=656&amp;table=block&amp;id=adac8c80-1e86-4775-b476-9913e591b7ad"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2Fe7f3cc17-40e1-48bc-9b03-23dd725190e1%2FUntitled.png?width=656&amp;table=block&amp;id=adac8c80-1e86-4775-b476-9913e591b7ad" style="width:656px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/e06b6c28a4c64b26a457073dc5d429de" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">提出了地理 SAM（GeoSAM），这是一种基于 SAM 的新型框架，利用零点学习的密集视觉提示和预训练 CNN 分割模型的稀疏视觉提示实施微调策略。用于做人行道分割，二分类。</span></span></p></div><h1 id="https://www.notion.so/9b27ed66650e4de38c3d3ce82ceefa38" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/9b27ed66650e4de38c3d3ce82ceefa38"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SAM for Mobile Applications</strong></strong></span></span></h1><blockquote id="https://www.notion.so/c993f5e7b47c4a909dc9e18a55970da1" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/2306.12156.pdf">https://arxiv.org/pdf/2306.12156.pdf</a></span></span><div id="https://www.notion.so/2c98a1a783814ae18413b0f7af78a7dd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">code：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/CASIA-IVA-Lab/FastSAM">https://github.com/CASIA-IVA-Lab/FastSAM</a></span></span></p></div></blockquote><div id="https://www.notion.so/7b1e1848ec524626a4891e3547703ae7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">FastSAM 基于 YOLOv8-seg 实现，它比 SAM 快50倍，且训练数据只有SAM的1/50，同时运行速度不受 point 输入数量的影响</span></span></p></div><div id="https://www.notion.so/207b40eae5824ddfa5cecec20de4ccbd" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F927fcbb0-d929-4452-90ce-d880f38fc29d%2FUntitled.png?width=1017&amp;table=block&amp;id=207b40ea-e582-4ddf-a5ce-cec20de4ccbd"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F927fcbb0-d929-4452-90ce-d880f38fc29d%2FUntitled.png?width=1017&amp;table=block&amp;id=207b40ea-e582-4ddf-a5ce-cec20de4ccbd" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/062e616454a94353860ec510cfbe6fb4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><blockquote id="https://www.notion.so/2d060ad6f1c543789db2dc35b5b62711" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">paper：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/pdf/2306.14289.pdf">https://arxiv.org/pdf/2306.14289.pdf</a></span></span><div id="https://www.notion.so/323c1f5e0ffe4cb99e999b6d5e7357ce" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">code：</span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://github.com/ChaoningZhang/MobileSAM">https://github.com/ChaoningZhang/MobileSAM</a></span></span></p></div></blockquote><div id="https://www.notion.so/16038d32b40d4c3f9bc8fbaa3c357999" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">MobileSAM</code></span><span class="SemanticString">：将原始 SAM 中的图像编码器 ViT-H 的知识蒸馏到一个轻量化的图像编码器中，该编码器可以自动与原始 SAM 中的 Mask 解码器兼容。训练可以在不到一天的时间内在单个 GPU 上完成，它比原始 SAM 小60多倍，但性能与原始 SAM 相当。因为比FastSAM快，所以叫FasterSAM。</span></span></p></div><div id="https://www.notion.so/f978c2e234e04434afd4ca5113ce43e8" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F9f75722d-20b6-4123-8359-babd397c03f6%2FUntitled.png?width=794&amp;table=block&amp;id=f978c2e2-34e0-4434-afd4-ca5113ce43e8"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Ff3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2%2F9f75722d-20b6-4123-8359-babd397c03f6%2FUntitled.png?width=794&amp;table=block&amp;id=f978c2e2-34e0-4434-afd4-ca5113ce43e8" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h1 id="https://www.notion.so/1ad48fe1555548ecb33e8b18ac2af3d0" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/1ad48fe1555548ecb33e8b18ac2af3d0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">微调</strong></span></span></h1><div id="https://www.notion.so/e86b5efc0dbc487d9146f30ba19c66b2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">微调主要用于三个基本设置：</span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/9236d64f17da4f6c9dc4ddeb3c11cf33" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">提高模型在特定任务上的性能（例如开放世界物体检测，</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">Grounding-DINO</code></span><span class="SemanticString">）;</span></span></li><li id="https://www.notion.so/e6b9424d1e594cd480f1dc83d7de360a" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">提高模型在某一特定能力上的性能（例如视觉定位）;</span></span></li><li id="https://www.notion.so/e9fc2707dadf48d39d72462366889f8f" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">调整模型以解决不同的下游视觉任务（例如</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">InstructBLIP</code></span><span class="SemanticString">）；</span></span></li><li id="https://www.notion.so/3bf353bbbcb740669ac344842e281c86" class="NumberedList" value="4"><span class="SemanticStringArray"><span class="SemanticString">调整参数，修改image encoder的结构，加入Adapter。</span></span></li></ol><div id="https://www.notion.so/105053dfda064f25ad132ed7b3475b08" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">首先，许多工作展示，即使只采用线性探测，也可以提高模型在特定任务上的性能。因此，特定任务的数据集（例如</span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">ImageNet</code></span><span class="SemanticString">）是可以用来改善预训练模型的特定任务性能。其次，一些工作已经利用预训练的视觉语言模型，通过在定位数据集上微调模型来进行定位任务。</span></span></p></div><div id="https://www.notion.so/7b5d7ed6552e48adabb7d88a623450e3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">例如，谷歌的一篇 OVD 工作 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">OWL-ViT</code></span><span class="SemanticString">，将 CLIP 预训练模型去掉 Token Pooling+projection 和 Image projection，加上一个新的 Linear Projection 作为分类头与文本进行匹配，学习出每个 Patch 的语义信息。此外在将 Patch 的表征经过 MLP head 回归出相应检测狂。通过 Patch 的语义特征与 BBox 的位置最终获得目标检测框。最后，像 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">InstructBLIP</code></span><span class="SemanticString"> 则将视觉数据集转换为指导调整数据集，使视觉语言模型能够用于下游任务。</span></span></p></div><div id="https://www.notion.so/42d2fe9e9cc64d318fa3e55aa8756a9a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/9718165dffcd41e09ba33c2941c2ee3a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/9718165dffcd41e09ba33c2941c2ee3a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">总结</strong></span></span></h1><div id="https://www.notion.so/b39d3aeba90e42098b91ce0f6a5b148a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">具有对多种模式（包括自然语言和视觉）基础理解的模型对于开发能有效感知和推理现实世界的AI系统至关重要。今天主要为大家概括了视觉和语言基础模型，重点关注了它们的架构类型、训练目标、下游任务适应性和提示设计。</span></span></p></div><div id="https://www.notion.so/72074d0c75a74ac68555b4d4f70a9e8a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">多模态理解</strong></span><span class="SemanticString">：我们提供了对文本提示、视觉提示和异构模态模型的系统分类。这些模型不仅涵盖了自然语言，还包括了视觉和其他感知模式的理解。</span></span></p></div><div id="https://www.notion.so/8299622dbe3b4c21a191049ead087654" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">应用广泛性</strong></span><span class="SemanticString">：这些模型在各种视觉任务中的应用非常广泛，包括零样本识别和定位能力、关于图像或视频的视觉对话、跨模态和医疗数据理解。</span></span></p></div><div id="https://www.notion.so/1e491b79eefe4ecaa23d597df0362ecc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">通用模型</strong></span><span class="SemanticString">：视觉中的基础模型可以作为通用模型来解决多个任务。当与大型语言模型相结合时，它们促生了可以在复杂环境中持续学习和导航的基础实体代理。</span></span></p></div><div id="https://www.notion.so/c066d57420da4c4fa74fb2a7e1045d1b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">整体而言，基础视觉和语言模型的研究不仅深入了解了各种架构和训练目标，还展示了这些模型在多个领域和应用中的潜力。通过集成文本、视觉和其他模态的理解，这些模型促进了机器人技术和现实世界任务的进展。然而，还需要进一步的研究来充分挖掘这些模型的潜力，并解决一些存在的挑战和局限性。</span></span></p></div></article>
  <footer class="Footer">
  <div>&copy; Simon’s Blogs 2022</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>